[{"votes": "432", "answers": [{"answer_content": "Units:\n\nThe amount of \"neurons\", or \"cells\", or whatever the layer has inside it.   \n\nIt's a property of each layer, and yes, it's related to the output shape (as we will see later). In your picture, except for the input layer, which is conceptually different from other layers, you have: \n\nHidden layer 1: 4 units (4 neurons)   \nHidden layer 2: 4 units     \nLast layer: 1 unit\n\nShapes\nShapes are consequences of the model's configuration. Shapes are tuples representing how many elements an array or tensor has in each dimension. \nEx: a shape (30,4,10) means an array or tensor with 3 dimensions, containing 30 elements in the first dimension, 4 in the second and 10 in the third, totaling 30*4*10 = 1200 elements or numbers.   \nThe input shape\nWhat flows between layers are tensors. Tensors can be seen as matrices, with shapes.   \nIn Keras, the input layer itself is not a layer, but a tensor. It's the starting tensor you send to the first hidden layer. This tensor must have the same shape as your training data. \nExample: if you have 30 images of 50x50 pixels in RGB (3 channels), the shape of your input data is (30,50,50,3). Then your input layer tensor, must have this shape (see details in the \"shapes in keras\" section).   \nEach type of layer requires the input with a certain number of dimensions:\n\nDense layers require inputs as (batch_size, input_size)\n\nor (batch_size, optional,...,optional, input_size) \n\n2D convolutional layers need inputs as:\n\n\nif using channels_last: (batch_size, imageside1, imageside2, channels) \nif using channels_first: (batch_size, channels, imageside1, imageside2) \n\n1D convolutions and recurrent layers use (batch_size, sequence_length, features)\n\nDetails on how to prepare data for recurrent layers\n\n\nNow, the input shape is the only one you must define, because your model cannot know it. Only you know that, based on your training data.   \nAll the other shapes are calculated automatically based on the units and particularities of each layer.  \nRelation between shapes and units - The output shape\nGiven the input shape, all other shapes are results of layers calculations.   \nThe \"units\" of each layer will define the output shape (the shape of the tensor that is produced by the layer and that will be the input of the next layer).  \nEach type of layer works in a particular way. Dense layers have output shape based on \"units\", convolutional layers have output shape based on \"filters\". But it's always based on some layer property. (See the documentation for what each layer outputs)   \nLet's show what happens with \"Dense\" layers, which is the type shown in your graph.   \nA dense layer has an output shape of (batch_size,units). So, yes, units, the property of the layer, also defines the output shape.    \n\nHidden layer 1: 4 units, output shape: (batch_size,4).   \nHidden layer 2: 4 units, output shape: (batch_size,4).     \nLast layer: 1 unit, output shape: (batch_size,1).      \n\nWeights\nWeights will be entirely automatically calculated based on the input and the output shapes. Again, each type of layer works in a certain way. But the weights will be a matrix capable of transforming the input shape into the output shape by some mathematical operation. \nIn a dense layer, weights multiply all inputs. It's a matrix with one column per input and one row per unit, but this is often not important for basic works. \nIn the image, if each arrow had a multiplication number on it, all numbers together would form the weight matrix.\nShapes in Keras\nEarlier, I gave an example of 30 images, 50x50 pixels and 3 channels, having an input shape of (30,50,50,3).   \nSince the input shape is the only one you need to define, Keras will demand it in the first layer. \nBut in this definition, Keras ignores the first dimension, which is the batch size. Your model should be able to deal with any batch size, so you define only the other dimensions:\ninput_shape = (50,50,3)\n    #regardless of how many images I have, each image has this shape        \n\nOptionally, or when it's required by certain kinds of models, you can pass the shape containing the batch size via batch_input_shape=(30,50,50,3) or batch_shape=(30,50,50,3). This limits your training possibilities to this unique batch size, so it should be used only when really required.\nEither way you choose, tensors in the model will have the batch dimension.\nSo, even if you used input_shape=(50,50,3), when keras sends you messages, or when you print the model summary, it will show (None,50,50,3).\nThe first dimension is the batch size, it's None because it can vary depending on how many examples you give for training. (If you defined the batch size explicitly, then the number you defined will appear instead of None)\nAlso, in advanced works, when you actually operate directly on the tensors (inside Lambda layers or in the loss function, for instance), the batch size dimension will be there.   \n\nSo, when defining the input shape, you ignore the batch size: input_shape=(50,50,3) \nWhen doing operations directly on tensors, the shape will be again (30,50,50,3) \nWhen keras sends you a message, the shape will be (None,50,50,3) or (30,50,50,3), depending on what type of message it sends you.   \n\nDim\nAnd in the end, what is dim?   \nIf your input shape has only one dimension, you don't need to give it as a tuple, you give input_dim as a scalar number. \nSo, in your model, where your input layer has 3 elements, you can use any of these two:   \n\ninput_shape=(3,) -- The comma is necessary when you have only one dimension    \ninput_dim = 3 \n\nBut when dealing directly with the tensors, often dim will refer to how many dimensions a tensor has. For instance a tensor with shape (25,10909) has 2 dimensions. \n\nDefining your image in Keras\nKeras has two ways of doing it, Sequential models, or the functional API Model. I don't like using the sequential model, later you will have to forget it anyway because you will want models with branches.  \nPS: here I ignored other aspects, such as activation functions.\nWith the Sequential model:\nfrom keras.models import Sequential  \nfrom keras.layers import *  \n\nmodel = Sequential()    \n\n#start from the first hidden layer, since the input is not actually a layer   \n#but inform the shape of the input, with 3 elements.    \nmodel.add(Dense(units=4,input_shape=(3,))) #hidden layer 1 with input\n\n#further layers:    \nmodel.add(Dense(units=4)) #hidden layer 2\nmodel.add(Dense(units=1)) #output layer   \n\nWith the functional API Model:\nfrom keras.models import Model   \nfrom keras.layers import * \n\n#Start defining the input tensor:\ninpTensor = Input((3,))   \n\n#create the layers and pass them the input tensor to get the output tensor:    \nhidden1Out = Dense(units=4)(inpTensor)    \nhidden2Out = Dense(units=4)(hidden1Out)    \nfinalOut = Dense(units=1)(hidden2Out)   \n\n#define the model's start and end points    \nmodel = Model(inpTensor,finalOut)\n\nShapes of the tensors\nRemember you ignore batch sizes when defining layers: \n\ninpTensor: (None,3) \nhidden1Out: (None,4) \nhidden2Out: (None,4) \nfinalOut: (None,1)", "answer_comment": ["One question about the input_shape= parameter remains: to which dimension the first value of the argument refers? I see things like input_shape=(728, ), so in my mind the first argument refers to columns (fixed) and second to rows (free to vary). But how does this sit with Python's row-major order of arrays?", "That comma does not create a second dimension. It's just python notation for creating a tuple that contains only one element. input_shape(728,) is the same as batch_input=(batch_size,728). This means that each sample has 728 values.", "@DanielM\u00f6ller: could you please elaborate a little bit what the difference between \"input elements\" and \"dimensions\" are? I would think that the graph above had a three-dimensional input layer, thus making dim=3, so I'm wondering what I'm missing here, because I see you write that the input is 1-dimensional...", "A vector has one dimension, but many elements. It has shape (n,) ---- A matrix has two dimensions, dimension 0 has m elements, dimension 1 has n elements, totaling m x n elements, shape (m,n). If you imagine a \"cube\" divided in little cubes, each little cube with data, this would be 3D, with m x n x o elements, shape (m,n,o).", "@Prince, the order matters. You can configure Keras to use data_format = 'channels_first' or data_format='channels_last'. I recommend using always channels last (Keras' default). It's more compatible with all other layers."], "answer_score": "685", "answer_code_list": ["input_shape = (50,50,3)\n    #regardless of how many images I have, each image has this shape        \n", "from keras.models import Sequential  \nfrom keras.layers import *  \n\nmodel = Sequential()    \n\n#start from the first hidden layer, since the input is not actually a layer   \n#but inform the shape of the input, with 3 elements.    \nmodel.add(Dense(units=4,input_shape=(3,))) #hidden layer 1 with input\n\n#further layers:    \nmodel.add(Dense(units=4)) #hidden layer 2\nmodel.add(Dense(units=1)) #output layer   \n", "from keras.models import Model   \nfrom keras.layers import * \n\n#Start defining the input tensor:\ninpTensor = Input((3,))   \n\n#create the layers and pass them the input tensor to get the output tensor:    \nhidden1Out = Dense(units=4)(inpTensor)    \nhidden2Out = Dense(units=4)(hidden1Out)    \nfinalOut = Dense(units=1)(hidden2Out)   \n\n#define the model's start and end points    \nmodel = Model(inpTensor,finalOut)\n"], "is_accepted": true}, {"answer_content": "Input Dimension Clarified:\nNot a direct answer, but I just realized that the term \"Input Dimension\" could be confusing, so be wary:\nThe word \"dimension\" alone can refer to:\na) The dimension of Input Data (or stream) such as # N of sensor axes to beam the time series signal, or RGB color channels (3):\u00a0 suggested term = \"Input Stream Dimension\"\nb) The total number / length of Input Features (or Input layer) (28 x 28 = 784 for the MINST color image) or 3000 in the FFT transformed Spectrum Values, or\n\"Input Layer / Input Feature Dimension\"\nc) The dimensionality (# of dimensions) of the input (typically 3D as expected in Keras LSTM) or (# of Rows of Samples, # of Sensors, # of Values..) 3 is the answer.\n\"N Dimensionality of Input\"\nd) The SPECIFIC Input Shape (eg. (30,50,50,3) in this unwrapped input image data, or (30, 2500, 3) if unwrapped\nKeras:\u00a0\u00a0\u00a0\u00a0\nIn Keras, input_dim refers to the Dimension of Input Layer / Number of Input Features\n\u00a0\u00a0\u00a0 model = Sequential()\n\u00a0\u00a0\u00a0 model.add(Dense(32, input_dim=784))\u00a0 #or 3 in the current posted example above\n\u00a0\u00a0\u00a0 model.add(Activation('relu')) \n\nIn Keras LSTM, it refers to the total Time Steps\nThe term has been very confusing, we live in a very confusing world!!\nI find one of the challenge in Machine Learning is to deal with different languages or dialects and terminologies (like if you have 5-8 highly different versions of English, then you need a very high proficiency to converse with different speakers). Probably this is the same in programming languages too.", "answer_comment": [], "answer_score": "23", "answer_code_list": ["\u00a0\u00a0\u00a0 model = Sequential()\n\u00a0\u00a0\u00a0 model.add(Dense(32, input_dim=784))\u00a0 #or 3 in the current posted example above\n\u00a0\u00a0\u00a0 model.add(Activation('relu')) \n"], "is_accepted": false}, {"answer_content": "Added this answer to elaborate on the input shape at the first layer.\nI created tow variation of the same layers\nCase 1:\nmodel =Sequential()\nmodel.add(Dense(15, input_shape=(5,3),activation=\"relu\", kernel_initializer=\"he_uniform\", kernel_regularizer=None,kernel_constraint=\"MaxNorm\"))\nmodel.add(Dense(32,activation=\"relu\"))\nmodel.add(Dense(8))\n\nCase 2:\nmodel1=Sequential()\nmodel1.add(Dense(15,input_shape=(15,),kernel_initializer=\"he_uniform\",kernel_constraint=\"MaxNorm\",kernel_regularizer=None,activation=\"relu\"))\nmodel1.add(Dense(32,activation=\"relu\"))\nmodel1.add(Dense(8))\nplot_model(model1,show_shapes=True)\n\nNow if plot these and take summary,-\nCase 1\n\n[![Case1 Model Summary][2]][2]\n[2]: https://i.sstatic.net/WXh9z.png\nCase 2\n\nsummary\n\nNow if you look closely , in the first case , input is two dimensional. Output of first layer generates one for each row x number of units.\nCase two is simpler , there is not such complexity each unit produces one output after activation.", "answer_comment": [], "answer_score": "1", "answer_code_list": ["model =Sequential()\nmodel.add(Dense(15, input_shape=(5,3),activation=\"relu\", kernel_initializer=\"he_uniform\", kernel_regularizer=None,kernel_constraint=\"MaxNorm\"))\nmodel.add(Dense(32,activation=\"relu\"))\nmodel.add(Dense(8))\n", "model1=Sequential()\nmodel1.add(Dense(15,input_shape=(15,),kernel_initializer=\"he_uniform\",kernel_constraint=\"MaxNorm\",kernel_regularizer=None,activation=\"relu\"))\nmodel1.add(Dense(32,activation=\"relu\"))\nmodel1.add(Dense(8))\nplot_model(model1,show_shapes=True)\n"], "is_accepted": false}], "views": "368k", "title": "Keras input explanation: input_shape, units, batch_size, dim, etc", "question_link": "https://stackoverflow.com/questions/44747343/keras-input-explanation-input-shape-units-batch-size-dim-etc", "question_content": "For any Keras layer (Layer class), can someone explain how to understand the difference between input_shape, units, dim, etc.?  \nFor example the doc says units specify the output shape of a layer. \nIn the image of the neural net below hidden layer1 has 4 units. Does this directly translate to the units attribute of the Layer object? Or does units in Keras equal the shape of every weight in the hidden layer times the number of units? \nIn short how does one understand/visualize the attributes of the model -  in particular the layers - with the image below?", "question_comment": []}]