[{"votes": "4181", "answers": [{"answer_content": "DataFrame.iterrows is a generator which yields both the index and row (as a Series):\nimport pandas as pd\n\ndf = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]})\ndf = df.reset_index()  # make sure indexes pair with number of rows\n\nfor index, row in df.iterrows():\n    print(row['c1'], row['c2'])\n\n10 100\n11 110\n12 120\n\n\nObligatory disclaimer from the documentation\n\nIterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed and can be avoided with one of the following approaches:\n\nLook for a vectorized solution: many operations can be performed using built-in methods or NumPy functions, (boolean) indexing, \u2026\nWhen you have a function that cannot work on the full DataFrame/Series at once, it is better to use apply() instead of iterating over the values. See the docs on function application.\nIf you need to do iterative manipulations on the values but performance is important, consider writing the inner loop with cython or numba. See the enhancing performance section for some examples of this approach.\n\n\nOther answers in this thread delve into greater depth on alternatives to iter* functions if you are interested to learn more.", "answer_comment": ["Although .iterrows() works, it is around 600x slower than the fastest technique. I speed test and demonstrate 13 ways to iterate over a Pandas DataFrame in my answer here. .iterrows() is the 2nd slowest. 11 of the 13 techniques are faster than .iterrows(), and most of them are still really easy once you see an example of how to do them."], "answer_score": "5433", "answer_code_list": ["import pandas as pd\n\ndf = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]})\ndf = df.reset_index()  # make sure indexes pair with number of rows\n\nfor index, row in df.iterrows():\n    print(row['c1'], row['c2'])\n", "10 100\n11 110\n12 120\n"], "is_accepted": true}, {"answer_content": "How to iterate over rows in a DataFrame in Pandas\n\nAnswer: DON'T*!\nIteration in Pandas is an anti-pattern and is something you should only do when you have exhausted every other option. You should not use any function with \"iter\" in its name for more than a few thousand rows or you will have to get used to a lot of waiting.\nDo you want to print a DataFrame? Use DataFrame.to_string().\nDo you want to compute something? In that case, search for methods in this order (list modified from here):\n\nVectorization\nCython routines\nList Comprehensions (vanilla for loop)\nDataFrame.apply(): i) \u00a0Reductions that can be performed in Cython, ii) Iteration in Python space\nitems() iteritems() (deprecated since v1.5.0)\nDataFrame.itertuples()\nDataFrame.iterrows()\n\niterrows and itertuples (both receiving many votes in answers to this question) should be used in very rare circumstances, such as generating row objects/nametuples for sequential processing, which is really the only thing these functions are useful for.\nAppeal to Authority\nThe documentation page on iteration has a huge red warning box that says:\n\nIterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed [...].\n\n* It's actually a little more complicated than \"don't\". df.iterrows() is the correct answer to this question, but \"vectorize your ops\" is the better one. I will concede that there are circumstances where iteration cannot be avoided (for example, some operations where the result depends on the value computed for the previous row). However, it takes some familiarity with the library to know when. If you're not sure whether you need an iterative solution, you probably don't. PS: To know more about my rationale for writing this answer, skip to the very bottom.\n\nFaster than Looping: Vectorization, Cython\nA good number of basic operations and computations are \"vectorised\" by pandas (either through NumPy, or through Cythonized functions). This includes arithmetic, comparisons, (most) reductions, reshaping (such as pivoting), joins, and groupby operations. Look through the documentation on Essential Basic Functionality to find a suitable vectorised method for your problem.\nIf none exists, feel free to write your own using custom Cython extensions.\n\nNext Best Thing: List Comprehensions*\nList comprehensions should be your next port of call if 1) there is no vectorized solution available, 2) performance is important, but not important enough to go through the hassle of cythonizing your code, and 3) you're trying to perform elementwise transformation on your code. There is a good amount of evidence to suggest that list comprehensions are sufficiently fast (and even sometimes faster) for many common Pandas tasks.\nThe formula is simple,\n# Iterating over one column - `f` is some function that processes your data\nresult = [f(x) for x in df['col']]\n\n# Iterating over two columns, use `zip`\nresult = [f(x, y) for x, y in zip(df['col1'], df['col2'])]\n\n# Iterating over multiple columns - same data type\nresult = [f(row[0], ..., row[n]) for row in df[['col1', ...,'coln']].to_numpy()]\n\n# Iterating over multiple columns - differing data type\nresult = [f(row[0], ..., row[n]) for row in zip(df['col1'], ..., df['coln'])]\n\nIf you can encapsulate your business logic into a function, you can use a list comprehension that calls it. You can make arbitrarily complex things work through the simplicity and speed of raw Python code.\nCaveats\nList comprehensions assume that your data is easy to work with - what that means is your data types are consistent and you don't have NaNs, but this cannot always be guaranteed.\n\nThe first one is more obvious, but when dealing with NaNs, prefer in-built pandas methods if they exist (because they have much better corner-case handling logic), or ensure your business logic includes appropriate NaN handling logic.\nWhen dealing with mixed data types you should iterate over zip(df['A'], df['B'], ...) instead of df[['A', 'B']].to_numpy() as the latter implicitly upcasts data to the most common type. As an example if A is numeric and B is string, to_numpy() will cast the entire array to string, which may not be what you want. Fortunately zipping your columns together is the most straightforward workaround to this.\n\n*Your mileage may vary for the reasons outlined in the Caveats section above.\n\nAn Obvious Example\nLet's demonstrate the difference with a simple example of adding two pandas columns A + B. This is a vectorizable operation, so it will be easy to contrast the performance of the methods discussed above.\n\nBenchmarking code, for your reference. The line at the bottom measures a function written in numpandas, a style of Pandas that mixes heavily with NumPy to squeeze out maximum performance. Writing numpandas code should be avoided unless you know what you're doing. Stick to the API where you can (i.e., prefer vec over vec_numpy).\nI should mention, however, that it isn't always this cut and dry. Sometimes the answer to \"what is the best method for an operation\" is \"it depends on your data\". My advice is to test out different approaches on your data before settling on one.\n\nMy Personal Opinion *\nMost of the analyses performed on the various alternatives to the iter family has been through the lens of performance. However, in most situations you will typically be working on a reasonably sized dataset (nothing beyond a few thousand or 100K rows) and performance will come second to simplicity/readability of the solution.\nHere is my personal preference when selecting a method to use for a problem.\nFor the novice:\n\nVectorization (when possible); apply(); List Comprehensions; itertuples()/iteritems(); iterrows(); Cython\n\nFor the more experienced:\n\nVectorization (when possible); apply(); List Comprehensions; Cython; itertuples()/iteritems(); iterrows()\n\nVectorization prevails as the most idiomatic method for any problem that can be vectorized. Always seek to vectorize! When in doubt, consult the docs, or look on Stack Overflow for an existing question on your particular task.\nI do tend to go on about how bad apply is in a lot of my posts, but I do concede it is easier for a beginner to wrap their head around what it's doing. Additionally, there are quite a few use cases for apply has explained in this post of mine.\nCython ranks lower down on the list because it takes more time and effort to pull off correctly. You will usually never need to write code with pandas that demands this level of performance that even a list comprehension cannot satisfy.\n* As with any personal opinion, please take with heaps of salt!\n\nFurther Reading\n\n10 Minutes to pandas, and Essential Basic Functionality - Useful links that introduce you to Pandas and its library of vectorized*/cythonized functions.\n\nEnhancing Performance - A primer from the documentation on enhancing standard Pandas operations\n\nAre for-loops in pandas really bad? When should I care? - a detailed write-up by me on list comprehensions and their suitability for various operations (mainly ones involving non-numeric data)\n\nWhen should I (not) want to use pandas apply() in my code? - apply is slow (but not as slow as the iter* family. There are, however, situations where one can (or should) consider apply as a serious alternative, especially in some GroupBy operations).\n\n\n* Pandas string methods are \"vectorized\" in the sense that they are specified on the series but operate on each element. The underlying mechanisms are still iterative, because string operations are inherently hard to vectorize.\n\nWhy I Wrote this Answer\nA common trend I notice from new users is to ask questions of the form \"How can I iterate over my df to do X?\". Showing code that calls iterrows() while doing something inside a for loop. Here is why. A new user to the library who has not been introduced to the concept of vectorization will likely envision the code that solves their problem as iterating over their data to do something. Not knowing how to iterate over a DataFrame, the first thing they do is Google it and end up here, at this question. They then see the accepted answer telling them how to, and they close their eyes and run this code without ever first questioning if iteration is the right thing to do.\nThe aim of this answer is to help new users understand that iteration is not necessarily the solution to every problem, and that better, faster and more idiomatic solutions could exist, and that it is worth investing time in exploring them. I'm not trying to start a war of iteration vs. vectorization, but I want new users to be informed when developing solutions to their problems with this library.\nAnd finally ... a TLDR to summarize this post", "answer_comment": ["Iterating could be a tool to help during debugging, say with ipdb, especially during initial development and understanding of edge cases.", "@GabrielStaples Look here: \"... You can pass a list of columns to [] to select columns in that order. ...\"", "I used a lot of what you said and ran with it. Check out these 13 techniques I came up with and the plot that shows their speed differences. Pure vectorization is 1400x faster. List comprehension is pretty good too!", "What is you want to perform some kind of IO on a dataframe, row by row. For example, if you need to send individual rows to some \"thing\", what other approach would you use other than iterrows. eg: Sending rows to a network socket, or Kafka, as individual records. You could perhaps use something like .apply, but that is quite a messy solution. It can be done without the use of global variables using a functor (class) but the resulting code is hard to understand.", "@FreelanceConsultant, in my answer, try techniques 10, 5, 9, and 1, in that order. All of those are faster than using iterrows(). In all of those techniques, you would replace my calculate_val() function call with your do_io() function call. I think that would work and technique 10 (10_list_comprehension_w_zip_and_direct_variable_assignment_passed_to_func) would be one of the fastest and easiest techniques possible."], "answer_score": "2502", "answer_code_list": ["# Iterating over one column - `f` is some function that processes your data\nresult = [f(x) for x in df['col']]\n\n# Iterating over two columns, use `zip`\nresult = [f(x, y) for x, y in zip(df['col1'], df['col2'])]\n\n# Iterating over multiple columns - same data type\nresult = [f(row[0], ..., row[n]) for row in df[['col1', ...,'coln']].to_numpy()]\n\n# Iterating over multiple columns - differing data type\nresult = [f(row[0], ..., row[n]) for row in zip(df['col1'], ..., df['coln'])]\n"], "is_accepted": false}, {"answer_content": "First consider if you really need to iterate over rows in a DataFrame. See cs95's answer for alternatives.\nIf you still need to iterate over rows, you can use methods below. Note some  important caveats which are not mentioned in any of the other answers.\n\nDataFrame.iterrows()\nfor index, row in df.iterrows():\n    print(row[\"c1\"], row[\"c2\"])\n\n\nDataFrame.itertuples()\nfor row in df.itertuples(index=True, name='Pandas'):\n    print(row.c1, row.c2)\n\n\n\nitertuples() is supposed to be faster than iterrows()\nBut be aware, according to the docs (pandas 0.24.2 at the moment):\n\niterrows: dtype might not match from row to row\n\nBecause iterrows returns a Series for each row, it does not preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). To preserve dtypes while iterating over the rows, it is better to use itertuples() which returns namedtuples of the values and which is generally much faster than iterrows()\n\n\n\n\niterrows: Do not modify rows\n\nYou should never modify something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect.\n\nUse DataFrame.apply() instead:\nnew_df = df.apply(lambda x: x * 2, axis=1)\n\n\nitertuples:\n\nThe column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. With a large number of columns (>255), regular tuples are returned.\n\n\n\nSee pandas docs on iteration for more details.", "answer_comment": ["Note: you can also say something like for row in df[['c1','c2']].itertuples(index=True, name=None): to include only certain columns in the row iterator.", "I don't know why, but using name=None make itertuples 50% faster in my use case.", "A raw for loop (my technique 2) is faster than iterrows() (my technique 1) as shown in my answer here. And itertuples() is 38x faster than iterrows(). In my \"Rules of thumb\" section in my answer, I propose that \"[iterrows()] should never be used. [It is] super slow and [has] no advantages whatsoever.\" I present 13 techniques, several of which are extremely fast. iterrows() is the 2nd slowest."], "answer_score": "578", "answer_code_list": ["for index, row in df.iterrows():\n    print(row[\"c1\"], row[\"c2\"])\n", "for row in df.itertuples(index=True, name='Pandas'):\n    print(row.c1, row.c2)\n", "new_df = df.apply(lambda x: x * 2, axis=1)\n"], "is_accepted": false}, {"answer_content": "You should use df.iterrows(). Though iterating row-by-row is not especially efficient since Series objects have to be created.", "answer_comment": ["I have done a bit of testing on the time consumption for df.iterrows(), df.itertuples(), and zip(df['a'], df['b']) and posted the result in the answer of another question: stackoverflow.com/a/34311080/2142098"], "answer_score": "255", "answer_code_list": [], "is_accepted": false}, {"answer_content": "While iterrows() is a good option, sometimes itertuples() can be much faster:\ndf = pd.DataFrame({'a': randn(1000), 'b': randn(1000),'N': randint(100, 1000, (1000)), 'x': 'x'})\n\n%timeit [row.a * 2 for idx, row in df.iterrows()]\n# => 10 loops, best of 3: 50.3 ms per loop\n\n%timeit [row[1] * 2 for row in df.itertuples()]\n# => 1000 loops, best of 3: 541 \u00b5s per loop", "answer_comment": ["Much of the time difference in your two examples seems like it is due to the fact that you appear to be using label-based indexing for the .iterrows() command and integer-based indexing for the .itertuples() command.", "@AbeMiessler iterrows() boxes each row of data into a Series, whereas itertuples()does not.", "I get a >50 times increase as well i.sstatic.net/HBe9o.png (while changing to attr accessor in the second run)."], "answer_score": "189", "answer_code_list": ["df = pd.DataFrame({'a': randn(1000), 'b': randn(1000),'N': randint(100, 1000, (1000)), 'x': 'x'})\n\n%timeit [row.a * 2 for idx, row in df.iterrows()]\n# => 10 loops, best of 3: 50.3 ms per loop\n\n%timeit [row[1] * 2 for row in df.itertuples()]\n# => 1000 loops, best of 3: 541 \u00b5s per loop\n"], "is_accepted": false}, {"answer_content": "You can use the df.iloc function as follows:\nfor i in range(0, len(df)):\n    print(df.iloc[i]['c1'], df.iloc[i]['c2'])", "answer_comment": ["This is the only valid technique I know of if you want to preserve the data types, and also refer to columns by name.  itertuples preserves data types, but gets rid of any name it doesn't like.  iterrows does the opposite."], "answer_score": "173", "answer_code_list": ["for i in range(0, len(df)):\n    print(df.iloc[i]['c1'], df.iloc[i]['c2'])\n"], "is_accepted": false}, {"answer_content": "You can also use df.apply() to iterate over rows and access multiple columns for a function.\ndocs: DataFrame.apply()\ndef valuation_formula(x, y):\n    return x * y * 0.5\n\ndf['price'] = df.apply(lambda row: valuation_formula(row['x'], row['y']), axis=1)\n\nNote that axis=1 here is the same as axis='columns', and is used do apply the function to each row instead of to each column. If unspecified, the default behavior is to apply to the function to each column.", "answer_comment": ["Notice that apply doesn't \"iteratite\" over rows, rather it applies a function row-wise. The above code wouldn't work if you really do need iterations and indeces, for instance when comparing values across different rows (in that case you can do nothing but iterating).", "@gented, that's not exactly true. To get access to values in a previous row, for instance, you can simply add a new column containing previous-row values, like this: dataframe[\"val_previous\"] = dataframe[\"val\"].shift(1). Then, you could access this val_previous variable in a given row using this answer."], "answer_score": "136", "answer_code_list": ["def valuation_formula(x, y):\n    return x * y * 0.5\n\ndf['price'] = df.apply(lambda row: valuation_formula(row['x'], row['y']), axis=1)\n"], "is_accepted": false}, {"answer_content": "How to iterate efficiently\nIf you really have to iterate a Pandas DataFrame, you will probably want to avoid using iterrows(). There are different methods, and the usual iterrows() is far from being the best. `itertuples()`` can be 100 times faster.\nIn short:\n\nAs a general rule, use df.itertuples(name=None). In particular, when you have a fixed number columns and fewer than 255 columns. See bullet (3) below.\nOtherwise, use df.itertuples(), except if your columns have special characters such as spaces or -. See bullet (2) below.\nIt is possible to use itertuples() even if your dataframe has strange columns, by using the last example below. See bullet (4) below.\nOnly use iterrows() if you cannot use any of the previous solutions. See bullet (1) below.\n\nDifferent methods to iterate over rows in a Pandas DataFrame:\nFirst, for use in all examples below, generate a random dataframe with a million rows and 4 columns, like this:\ndf = pd.DataFrame(np.random.randint(0, 100, size=(1000000, 4)), columns=list('ABCD'))\nprint(df)\n\nThe output of all of these examples is shown at the bottom.\n\nThe usual iterrows() is convenient, but damn slow:\nstart_time = time.clock()\nresult = 0\nfor _, row in df.iterrows():\n    result += max(row['B'], row['C'])\n\ntotal_elapsed_time = round(time.clock() - start_time, 2)\nprint(\"1. Iterrows done in {} seconds, result = {}\".format(total_elapsed_time, result))\n\n\nUsing the default named itertuples() is already much faster, but it doesn't work with column names such as My Col-Name is very Strange (you should avoid this method if your columns are repeated or if a column name cannot be simply converted to a Python variable name).:\nstart_time = time.clock()\nresult = 0\nfor row in df.itertuples(index=False):\n    result += max(row.B, row.C)\n\ntotal_elapsed_time = round(time.clock() - start_time, 2)\nprint(\"2. Named Itertuples done in {} seconds, result = {}\".format(total_elapsed_time, result))\n\n\nUsing nameless itertuples() by setting name=None is even faster, but not really convenient, as you have to define a variable per column.\nstart_time = time.clock()\nresult = 0\nfor(_, col1, col2, col3, col4) in df.itertuples(name=None):\n    result += max(col2, col3)\n\ntotal_elapsed_time = round(time.clock() - start_time, 2)\nprint(\"3. Itertuples done in {} seconds, result = {}\".format(total_elapsed_time, result))\n\n\nFinally, using polyvalent itertuples() is slower than the previous example, but you do not have to define a variable per column and it works with column names such as My Col-Name is very Strange.\nstart_time = time.clock()\nresult = 0\nfor row in df.itertuples(index=False):\n    result += max(row[df.columns.get_loc('B')], row[df.columns.get_loc('C')])\n\ntotal_elapsed_time = round(time.clock() - start_time, 2)\nprint(\"4. Polyvalent Itertuples working even with special characters in the column name done in {} seconds, result = {}\".format(total_elapsed_time, result))\n\n\n\nOutput of all code and examples above:\n         A   B   C   D\n0       41  63  42  23\n1       54   9  24  65\n2       15  34  10   9\n3       39  94  82  97\n4        4  88  79  54\n...     ..  ..  ..  ..\n999995  48  27   4  25\n999996  16  51  34  28\n999997   1  39  61  14\n999998  66  51  27  70\n999999  51  53  47  99\n\n[1000000 rows x 4 columns]\n\n1. Iterrows done in 104.96 seconds, result = 66151519\n2. Named Itertuples done in 1.26 seconds, result = 66151519\n3. Itertuples done in 0.94 seconds, result = 66151519\n4. Polyvalent Itertuples working even with special characters in the column name done in 2.94 seconds, result = 66151519\n\nPlot of these results from @Gabriel Staples in his answer here:\n\nSee also\n\nThis article is a very interesting comparison between iterrows() and itertuples()", "answer_comment": [], "answer_score": "79", "answer_code_list": ["df = pd.DataFrame(np.random.randint(0, 100, size=(1000000, 4)), columns=list('ABCD'))\nprint(df)\n", "start_time = time.clock()\nresult = 0\nfor _, row in df.iterrows():\n    result += max(row['B'], row['C'])\n\ntotal_elapsed_time = round(time.clock() - start_time, 2)\nprint(\"1. Iterrows done in {} seconds, result = {}\".format(total_elapsed_time, result))\n", "start_time = time.clock()\nresult = 0\nfor row in df.itertuples(index=False):\n    result += max(row.B, row.C)\n\ntotal_elapsed_time = round(time.clock() - start_time, 2)\nprint(\"2. Named Itertuples done in {} seconds, result = {}\".format(total_elapsed_time, result))\n", "start_time = time.clock()\nresult = 0\nfor(_, col1, col2, col3, col4) in df.itertuples(name=None):\n    result += max(col2, col3)\n\ntotal_elapsed_time = round(time.clock() - start_time, 2)\nprint(\"3. Itertuples done in {} seconds, result = {}\".format(total_elapsed_time, result))\n", "start_time = time.clock()\nresult = 0\nfor row in df.itertuples(index=False):\n    result += max(row[df.columns.get_loc('B')], row[df.columns.get_loc('C')])\n\ntotal_elapsed_time = round(time.clock() - start_time, 2)\nprint(\"4. Polyvalent Itertuples working even with special characters in the column name done in {} seconds, result = {}\".format(total_elapsed_time, result))\n", "         A   B   C   D\n0       41  63  42  23\n1       54   9  24  65\n2       15  34  10   9\n3       39  94  82  97\n4        4  88  79  54\n...     ..  ..  ..  ..\n999995  48  27   4  25\n999996  16  51  34  28\n999997   1  39  61  14\n999998  66  51  27  70\n999999  51  53  47  99\n\n[1000000 rows x 4 columns]\n\n1. Iterrows done in 104.96 seconds, result = 66151519\n2. Named Itertuples done in 1.26 seconds, result = 66151519\n3. Itertuples done in 0.94 seconds, result = 66151519\n4. Polyvalent Itertuples working even with special characters in the column name done in 2.94 seconds, result = 66151519\n"], "is_accepted": false}, {"answer_content": "I was looking for How to iterate on rows and columns and ended here so:\nfor i, row in df.iterrows():\n    for j, column in row.iteritems():\n        print(column)", "answer_comment": [], "answer_score": "58", "answer_code_list": ["for i, row in df.iterrows():\n    for j, column in row.iteritems():\n        print(column)\n"], "is_accepted": false}, {"answer_content": "We have multiple options to do the same, and lots of folks have shared their answers.\nI found the below two methods easy and efficient to do:\n\nDataFrame.iterrows()\nDataFrame.itertuples()\n\nExample:\n import pandas as pd\n inp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]\n df = pd.DataFrame(inp)\n print (df)\n\n # With the iterrows method\n\n for index, row in df.iterrows():\n     print(row[\"c1\"], row[\"c2\"])\n\n # With the itertuples method\n\n for row in df.itertuples(index=True, name='Pandas'):\n     print(row.c1, row.c2)\n\nNote: itertuples() is supposed to be faster than iterrows()", "answer_comment": [], "answer_score": "48", "answer_code_list": [" import pandas as pd\n inp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]\n df = pd.DataFrame(inp)\n print (df)\n\n # With the iterrows method\n\n for index, row in df.iterrows():\n     print(row[\"c1\"], row[\"c2\"])\n\n # With the itertuples method\n\n for row in df.itertuples(index=True, name='Pandas'):\n     print(row.c1, row.c2)\n"], "is_accepted": false}, {"answer_content": "Key takeaways:\n\nUse vectorization.\nSpeed profile your code! Don't assume something is faster because you think it is faster; speed profile it and prove it is faster. The results may surprise you.\n\nHow to iterate over Pandas DataFrames without iterating\nAfter several weeks of working on this answer, here's what I've come up with:\nHere are 13 techniques for iterating over Pandas DataFrames. As you can see, the time it takes varies dramatically. The fastest technique is ~1363x faster than the slowest technique! The key takeaway, as @cs95 says here, is don't iterate! Use vectorization (\"array programming\") instead. All this really means is that you should use the arrays directly in mathematical formulas rather than trying to manually iterate over the arrays. The underlying objects must support this, of course, but both Numpy and Pandas do.\nThere are many ways to use vectorization in Pandas, which you can see in the plot and in my example code below. When using the arrays directly, the underlying looping still takes place, but in (I think) very optimized underlying C code rather than through raw Python.\nResults\n13 techniques, numbered 1 to 13, were tested. The technique number and name is underneath each bar. The total calculation time is above each bar. Underneath that is the multiplier to show how much longer it took than the fastest technique to the far right:\nFrom pandas_dataframe_iteration_vs_vectorization_vs_list_comprehension_speed_tests.svg in my eRCaGuy_hello_world repo (produced by this code).\n\nSummary\nList comprehension and vectorization (possibly with boolean indexing) are all you really need.\nUse list comprehension (good) and vectorization (best). Pure vectorization I think is always possible, but may take extra work in complicated calculations. Search this answer for \"boolean indexing\", \"boolean array\", and \"boolean mask\" (all three are the same thing) to see some of the more complicated cases where pure vectorization can thereby be used.\nHere are the 13 techniques, listed in order of fastest first to slowest last. I recommend never using the last (slowest) 3 to 4 techniques.\n\nTechnique 8: 8_pure_vectorization__with_df.loc[]_boolean_array_indexing_for_if_statment_corner_case\nTechnique 6: 6_vectorization__with_apply_for_if_statement_corner_case\nTechnique 7: 7_vectorization__with_list_comprehension_for_if_statment_corner_case\nTechnique 11: 11_list_comprehension_w_zip_and_direct_variable_assignment_calculated_in_place\nTechnique 10: 10_list_comprehension_w_zip_and_direct_variable_assignment_passed_to_func\nTechnique 12: 12_list_comprehension_w_zip_and_row_tuple_passed_to_func\nTechnique 5: 5_itertuples_in_for_loop\nTechnique 13: 13_list_comprehension_w__to_numpy__and_direct_variable_assignment_passed_to_func\nTechnique 9: 9_apply_function_with_lambda\nTechnique 1: 1_raw_for_loop_using_regular_df_indexing\nTechnique 2: 2_raw_for_loop_using_df.loc[]_indexing\nTechnique 4: 4_iterrows_in_for_loop\nTechnique 3: 3_raw_for_loop_using_df.iloc[]_indexing\n\nRules of thumb:\n\nTechniques 3, 4, and 2 should never be used. They are super slow and have no advantages whatsoever. Keep in mind though: it's not the indexing technique, such as .loc[] or .iloc[] that makes these techniques bad, but rather, it's the for loop they are in that makes them bad! I use .loc[] inside the fastest (pure vectorization) approach, for instance! So, here are the 3 slowest techniques which should never be used:\n\n3_raw_for_loop_using_df.iloc[]_indexing\n4_iterrows_in_for_loop\n2_raw_for_loop_using_df.loc[]_indexing\n\n\nTechnique 1_raw_for_loop_using_regular_df_indexing should never be used either, but if you're going to use a raw for loop, it's faster than the others.\nThe .apply() function (9_apply_function_with_lambda) is ok, but generally speaking, I'd avoid it too. Technique 6_vectorization__with_apply_for_if_statement_corner_case did perform better than 7_vectorization__with_list_comprehension_for_if_statment_corner_case, however, which is interesting.\nList comprehension is great! It's not the fastest, but it is easy to use and very fast!\n\nThe nice thing about it is that it can be used with any function that is intended to work on individual values, or array values. And this means you could have really complicated if statements and things inside the function. So, the tradeoff here is that it gives you great versatility with really readable and re-usable code by using external calculation functions, while still giving you great speed!\n\n\nVectorization is the fastest and best, and what you should use whenever the equation is simple. You can optionally use something like .apply() or list comprehension on just the more-complicated portions of the equation, while still easily using vectorization for the rest.\nPure vectorization is the absolute fastest and best, and what you should use if you are willing to put in the effort to make it work.\n\nFor simple cases, it's what you should use.\nFor complicated cases, if statements, etc., pure vectorization can be made to work too, through boolean indexing, but can add extra work and can decrease readability to do so. So, you can optionally use list comprehension (usually the best) or .apply() (generally slower, but not always) for just those edge cases instead, while still using vectorization for the rest of the calculation. Ex: see techniques 7_vectorization__with_list_comprehension_for_if_statment_corner_case and 6_vectorization__with_apply_for_if_statement_corner_case.\n\n\n\nThe test data\nAssume we have the following Pandas DataFrame. It has 2 million rows with 4 columns (A, B, C, and D), each with random values from -1000 to 1000:\ndf =\n           A    B    C    D\n0       -365  842  284 -942\n1        532  416 -102  888\n2        397  321 -296 -616\n3       -215  879  557  895\n4        857  701 -157  480\n...      ...  ...  ...  ...\n1999995 -101 -233 -377 -939\n1999996 -989  380  917  145\n1999997 -879  333 -372 -970\n1999998  738  982 -743  312\n1999999 -306 -103  459  745\n\nI produced this DataFrame like this:\nimport numpy as np\nimport pandas as pd\n\n# Create an array (numpy list of lists) of fake data\nMIN_VAL = -1000\nMAX_VAL = 1000\n# NUM_ROWS = 10_000_000\nNUM_ROWS = 2_000_000  # default for final tests\n# NUM_ROWS = 1_000_000\n# NUM_ROWS = 100_000\n# NUM_ROWS = 10_000  # default for rapid development & initial tests\nNUM_COLS = 4\ndata = np.random.randint(MIN_VAL, MAX_VAL, size=(NUM_ROWS, NUM_COLS))\n\n# Now convert it to a Pandas DataFrame with columns named \"A\", \"B\", \"C\", and \"D\"\ndf_original = pd.DataFrame(data, columns=[\"A\", \"B\", \"C\", \"D\"])\nprint(f\"df = \\n{df_original}\")\n\nThe test equation/calculation\nI wanted to demonstrate that all of these techniques are possible on non-trivial functions or equations, so I intentionally made the equation they are calculating require:\n\nif statements\ndata from multiple columns in the DataFrame\ndata from multiple rows in the DataFrame\n\nThe equation we will be calculating for each row is this. I arbitrarily made it up, but I think it contains enough complexity that you will be able to expand on what I've done to perform any equation you want in Pandas with full vectorization:\n\nIn Python, the above equation can be written like this:\n# Calculate and return a new value, `val`, by performing the following equation:\nval = (\n    2 * A_i_minus_2\n    + 3 * A_i_minus_1\n    + 4 * A\n    + 5 * A_i_plus_1\n    # Python ternary operator; don't forget parentheses around the entire\n    # ternary expression!\n    + ((6 * B) if B > 0 else (60 * B))\n    + 7 * C\n    - 8 * D\n)\n\nAlternatively, you could write it like this:\n# Calculate and return a new value, `val`, by performing the following equation:\n\nif B > 0:\n    B_new = 6 * B\nelse:\n    B_new = 60 * B\n\nval = (\n    2 * A_i_minus_2\n    + 3 * A_i_minus_1\n    + 4 * A\n    + 5 * A_i_plus_1\n    + B_new\n    + 7 * C\n    - 8 * D\n)\n\nEither of those can be wrapped into a function. Ex:\ndef calculate_val(\n        A_i_minus_2,\n        A_i_minus_1,\n        A,\n        A_i_plus_1,\n        B,\n        C,\n        D):\n    val = (\n        2 * A_i_minus_2\n        + 3 * A_i_minus_1\n        + 4 * A\n        + 5 * A_i_plus_1\n        # Python ternary operator; don't forget parentheses around the\n        # entire ternary expression!\n        + ((6 * B) if B > 0 else (60 * B))\n        + 7 * C\n        - 8 * D\n    )\n    return val\n\nThe techniques\nThe full code is available to download and run in my python/pandas_dataframe_iteration_vs_vectorization_vs_list_comprehension_speed_tests.py file in my eRCaGuy_hello_world repo.\nHere is the code for all 13 techniques:\n\nTechnique 1: 1_raw_for_loop_using_regular_df_indexing\nval = [np.NAN]*len(df)\nfor i in range(len(df)):\n    if i < 2 or i > len(df)-2:\n        continue\n\n    val[i] = calculate_val(\n        df[\"A\"][i-2],\n        df[\"A\"][i-1],\n        df[\"A\"][i],\n        df[\"A\"][i+1],\n        df[\"B\"][i],\n        df[\"C\"][i],\n        df[\"D\"][i],\n    )\ndf[\"val\"] = val  # put this column back into the dataframe\n\n\nTechnique 2: 2_raw_for_loop_using_df.loc[]_indexing\nval = [np.NAN]*len(df)\nfor i in range(len(df)):\n    if i < 2 or i > len(df)-2:\n        continue\n\n    val[i] = calculate_val(\n        df.loc[i-2, \"A\"],\n        df.loc[i-1, \"A\"],\n        df.loc[i,   \"A\"],\n        df.loc[i+1, \"A\"],\n        df.loc[i,   \"B\"],\n        df.loc[i,   \"C\"],\n        df.loc[i,   \"D\"],\n    )\n\ndf[\"val\"] = val  # put this column back into the dataframe\n\n\nTechnique 3: 3_raw_for_loop_using_df.iloc[]_indexing\n# column indices\ni_A = 0\ni_B = 1\ni_C = 2\ni_D = 3\n\nval = [np.NAN]*len(df)\nfor i in range(len(df)):\n    if i < 2 or i > len(df)-2:\n        continue\n\n    val[i] = calculate_val(\n        df.iloc[i-2, i_A],\n        df.iloc[i-1, i_A],\n        df.iloc[i,   i_A],\n        df.iloc[i+1, i_A],\n        df.iloc[i,   i_B],\n        df.iloc[i,   i_C],\n        df.iloc[i,   i_D],\n    )\n\ndf[\"val\"] = val  # put this column back into the dataframe\n\n\nTechnique 4: 4_iterrows_in_for_loop\nval = [np.NAN]*len(df)\nfor index, row in df.iterrows():\n    if index < 2 or index > len(df)-2:\n        continue\n\n    val[index] = calculate_val(\n        df[\"A\"][index-2],\n        df[\"A\"][index-1],\n        row[\"A\"],\n        df[\"A\"][index+1],\n        row[\"B\"],\n        row[\"C\"],\n        row[\"D\"],\n    )\n\ndf[\"val\"] = val  # put this column back into the dataframe\n\n\n\nFor all of the next examples, we must first prepare the dataframe by adding columns with previous and next values: A_(i-2), A_(i-1), and A_(i+1). These columns in the DataFrame will be named A_i_minus_2, A_i_minus_1, and A_i_plus_1, respectively:\ndf_original[\"A_i_minus_2\"] = df_original[\"A\"].shift(2)  # val at index i-2\ndf_original[\"A_i_minus_1\"] = df_original[\"A\"].shift(1)  # val at index i-1\ndf_original[\"A_i_plus_1\"] = df_original[\"A\"].shift(-1)  # val at index i+1\n\n# Note: to ensure that no partial calculations are ever done with rows which\n# have NaN values due to the shifting, we can either drop such rows with\n# `.dropna()`, or set all values in these rows to NaN. I'll choose the latter\n# so that the stats that will be generated with the techniques below will end\n# up matching the stats which were produced by the prior techniques above. ie:\n# the number of rows will be identical to before.\n#\n# df_original = df_original.dropna()\ndf_original.iloc[:2, :] = np.NAN   # slicing operators: first two rows,\n                                   # all columns\ndf_original.iloc[-1:, :] = np.NAN  # slicing operators: last row, all columns\n\nRunning the vectorized code just above to produce those 3 new columns took a total of 0.044961 seconds.\nNow on to the rest of the techniques:\n\nTechnique 5: 5_itertuples_in_for_loop\nval = [np.NAN]*len(df)\nfor row in df.itertuples():\n    val[row.Index] = calculate_val(\n        row.A_i_minus_2,\n        row.A_i_minus_1,\n        row.A,\n        row.A_i_plus_1,\n        row.B,\n        row.C,\n        row.D,\n    )\n\ndf[\"val\"] = val  # put this column back into the dataframe\n\n\nTechnique 6: 6_vectorization__with_apply_for_if_statement_corner_case\ndef calculate_new_column_b_value(b_value):\n    # Python ternary operator\n    b_value_new = (6 * b_value) if b_value > 0 else (60 * b_value)\n    return b_value_new\n\n# In this particular example, since we have an embedded `if-else` statement\n# for the `B` column, pure vectorization is less intuitive. So, first we'll\n# calculate a new `B` column using\n# **`apply()`**, then we'll use vectorization for the rest.\ndf[\"B_new\"] = df[\"B\"].apply(calculate_new_column_b_value)\n# OR (same thing, but with a lambda function instead)\n# df[\"B_new\"] = df[\"B\"].apply(lambda x: (6 * x) if x > 0 else (60 * x))\n\n# Now we can use vectorization for the rest. \"Vectorization\" in this case\n# means to simply use the column series variables in equations directly,\n# without manually iterating over them. Pandas DataFrames will handle the\n# underlying iteration automatically for you. You just focus on the math.\ndf[\"val\"] = (\n    2 * df[\"A_i_minus_2\"]\n    + 3 * df[\"A_i_minus_1\"]\n    + 4 * df[\"A\"]\n    + 5 * df[\"A_i_plus_1\"]\n    + df[\"B_new\"]\n    + 7 * df[\"C\"]\n    - 8 * df[\"D\"]\n)\n\n\nTechnique 7: 7_vectorization__with_list_comprehension_for_if_statment_corner_case\n# In this particular example, since we have an embedded `if-else` statement\n# for the `B` column, pure vectorization is less intuitive. So, first we'll\n# calculate a new `B` column using **list comprehension**, then we'll use\n# vectorization for the rest.\ndf[\"B_new\"] = [\n    calculate_new_column_b_value(b_value) for b_value in df[\"B\"]\n]\n\n# Now we can use vectorization for the rest. \"Vectorization\" in this case\n# means to simply use the column series variables in equations directly,\n# without manually iterating over them. Pandas DataFrames will handle the\n# underlying iteration automatically for you. You just focus on the math.\ndf[\"val\"] = (\n    2 * df[\"A_i_minus_2\"]\n    + 3 * df[\"A_i_minus_1\"]\n    + 4 * df[\"A\"]\n    + 5 * df[\"A_i_plus_1\"]\n    + df[\"B_new\"]\n    + 7 * df[\"C\"]\n    - 8 * df[\"D\"]\n)\n\n\nTechnique 8: 8_pure_vectorization__with_df.loc[]_boolean_array_indexing_for_if_statment_corner_case\nThis uses boolean indexing, AKA: a boolean mask, to accomplish the equivalent of the if statement in the equation. In this way, pure vectorization can be used for the entire equation, thereby maximizing performance and speed.\n# If statement to evaluate:\n#\n#     if B > 0:\n#         B_new = 6 * B\n#     else:\n#         B_new = 60 * B\n#\n# In this particular example, since we have an embedded `if-else` statement\n# for the `B` column, we can use some boolean array indexing through\n# `df.loc[]` for some pure vectorization magic.\n#\n# Explanation:\n#\n# Long:\n#\n# The format is: `df.loc[rows, columns]`, except in this case, the rows are\n# specified by a \"boolean array\" (AKA: a boolean expression, list of\n# booleans, or \"boolean mask\"), specifying all rows where `B` is > 0. Then,\n# only in that `B` column for those rows, set the value accordingly. After\n# we do this for where `B` is > 0, we do the same thing for where `B`\n# is <= 0, except with the other equation.\n#\n# Short:\n#\n# For all rows where the boolean expression applies, set the column value\n# accordingly.\n#\n# GitHub CoPilot first showed me this `.loc[]` technique.\n# See also the official documentation:\n# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html\n#\n# ===========================\n# 1st: handle the > 0 case\n# ===========================\ndf[\"B_new\"] = df.loc[df[\"B\"] > 0, \"B\"] * 6\n#\n# ===========================\n# 2nd: handle the <= 0 case, merging the results into the\n# previously-created \"B_new\" column\n# ===========================\n# - NB: this does NOT work; it overwrites and replaces the whole \"B_new\"\n#   column instead:\n#\n#       df[\"B_new\"] = df.loc[df[\"B\"] <= 0, \"B\"] * 60\n#\n# This works:\ndf.loc[df[\"B\"] <= 0, \"B_new\"] = df.loc[df[\"B\"] <= 0, \"B\"] * 60\n\n# Now use normal vectorization for the rest.\ndf[\"val\"] = (\n    2 * df[\"A_i_minus_2\"]\n    + 3 * df[\"A_i_minus_1\"]\n    + 4 * df[\"A\"]\n    + 5 * df[\"A_i_plus_1\"]\n    + df[\"B_new\"]\n    + 7 * df[\"C\"]\n    - 8 * df[\"D\"]\n)\n\n\nTechnique 9: 9_apply_function_with_lambda\ndf[\"val\"] = df.apply(\n    lambda row: calculate_val(\n        row[\"A_i_minus_2\"],\n        row[\"A_i_minus_1\"],\n        row[\"A\"],\n        row[\"A_i_plus_1\"],\n        row[\"B\"],\n        row[\"C\"],\n        row[\"D\"]\n    ),\n    axis='columns' # same as `axis=1`: \"apply function to each row\",\n                   # rather than to each column\n)\n\n\nTechnique 10: 10_list_comprehension_w_zip_and_direct_variable_assignment_passed_to_func\ndf[\"val\"] = [\n    # Note: you *could* do the calculations directly here instead of using a\n    # function call, so long as you don't have indented code blocks such as\n    # sub-routines or multi-line if statements.\n    #\n    # I'm using a function call.\n    calculate_val(\n        A_i_minus_2,\n        A_i_minus_1,\n        A,\n        A_i_plus_1,\n        B,\n        C,\n        D\n    ) for A_i_minus_2, A_i_minus_1, A, A_i_plus_1, B, C, D\n    in zip(\n        df[\"A_i_minus_2\"],\n        df[\"A_i_minus_1\"],\n        df[\"A\"],\n        df[\"A_i_plus_1\"],\n        df[\"B\"],\n        df[\"C\"],\n        df[\"D\"]\n    )\n]\n\n\nTechnique 11: 11_list_comprehension_w_zip_and_direct_variable_assignment_calculated_in_place\ndf[\"val\"] = [\n    2 * A_i_minus_2\n    + 3 * A_i_minus_1\n    + 4 * A\n    + 5 * A_i_plus_1\n    # Python ternary operator; don't forget parentheses around the entire\n    # ternary expression!\n    + ((6 * B) if B > 0 else (60 * B))\n    + 7 * C\n    - 8 * D\n    for A_i_minus_2, A_i_minus_1, A, A_i_plus_1, B, C, D\n    in zip(\n        df[\"A_i_minus_2\"],\n        df[\"A_i_minus_1\"],\n        df[\"A\"],\n        df[\"A_i_plus_1\"],\n        df[\"B\"],\n        df[\"C\"],\n        df[\"D\"]\n    )\n]\n\n\nTechnique 12: 12_list_comprehension_w_zip_and_row_tuple_passed_to_func\ndf[\"val\"] = [\n    calculate_val(\n        row[0],\n        row[1],\n        row[2],\n        row[3],\n        row[4],\n        row[5],\n        row[6],\n    ) for row\n    in zip(\n        df[\"A_i_minus_2\"],\n        df[\"A_i_minus_1\"],\n        df[\"A\"],\n        df[\"A_i_plus_1\"],\n        df[\"B\"],\n        df[\"C\"],\n        df[\"D\"]\n    )\n]\n\n\nTechnique 13: 13_list_comprehension_w__to_numpy__and_direct_variable_assignment_passed_to_func\ndf[\"val\"] = [\n    # Note: you *could* do the calculations directly here instead of using a\n    # function call, so long as you don't have indented code blocks such as\n    # sub-routines or multi-line if statements.\n    #\n    # I'm using a function call.\n    calculate_val(\n        A_i_minus_2,\n        A_i_minus_1,\n        A,\n        A_i_plus_1,\n        B,\n        C,\n        D\n    ) for A_i_minus_2, A_i_minus_1, A, A_i_plus_1, B, C, D\n        # Note: this `[[...]]` double-bracket indexing is used to select a\n        # subset of columns from the dataframe. The inner `[]` brackets\n        # create a list from the column names within them, and the outer\n        # `[]` brackets accept this list to index into the dataframe and\n        # select just this list of columns, in that order.\n        # - See the official documentation on it here:\n        #   https://pandas.pydata.org/docs/user_guide/indexing.html#basics\n        #   - Search for the phrase \"You can pass a list of columns to [] to\n        #     select columns in that order.\"\n        #   - I learned this from this comment here:\n        #     https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas/55557758#comment136020567_55557758\n        # - One of the **list comprehension** examples in this answer here\n        #   uses `.to_numpy()` like this:\n        #   https://stackoverflow.com/a/55557758/4561887\n    in df[[\n        \"A_i_minus_2\",\n        \"A_i_minus_1\",\n        \"A\",\n        \"A_i_plus_1\",\n        \"B\",\n        \"C\",\n        \"D\"\n    ]].to_numpy()  # NB: `.values` works here too, but is deprecated. See:\n                   # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.values.html\n]\n\n\n\nHere are the results again:\n\nUsing the pre-shifted rows in the 4 for loop techniques as well\nI wanted to see if removing this if check and using the pre-shifted rows in the 4 for loop techniques would have much effect:\nif i < 2 or i > len(df)-2:\n    continue\n\n...so I created this file with those modifications: pandas_dataframe_iteration_vs_vectorization_vs_list_comprehension_speed_tests_mod.py. Search the file for \"MOD:\" to find the 4 new, modified techniques.\nIt had only a slight improvement. Here are the results of these 17 techniques now, with the 4 new ones having the word _MOD_ near the beginning of their name, just after their number. This is over 500k rows this time, not 2M:\n\nMore on .iterrtuples()\nThere are actually more nuances when using .itertuples(). To delve into some of those, read this answer by @Romain Capron. Here is a bar chart plot I made of his results:\n\nMy plotting code for his results is in python/pandas_plot_bar_chart_better_GREAT_AUTOLABEL_DATA.py in my eRCaGuy_hello_world repo.\nFuture work\nUsing Cython (Python compiled into C code), or just raw C functions called by Python, could be faster potentially, but I'm not going to do that for these tests. I'd only look into and speed test those options for big optimizations.\nI currently don't know Cython and don't feel the need to learn it. As you can see above, simply using pure vectorization properly already runs incredibly fast, processing 2 million rows in only 0.1 seconds, or 20 million rows per second.\nReferences\n\nA bunch of the official Pandas documentation, especially the DataFrame documentation here: https://pandas.pydata.org/pandas-docs/stable/reference/frame.html.\n\nThis excellent answer by @cs95 - this is where I learned in particular how to use list comprehension to iterate over a DataFrame.\n\nThis answer about itertuples(), by @Romain Capron - I studied it carefully and edited/formatted it.\n\nAll of this is my own code, but I want to point out that I had dozens of chats with GitHub Copilot (mostly), Bing AI, and ChatGPT in order to figure out many of these techniques and debug my code as I went.\n\nBing Chat produced the pretty LaTeX equation for me, with the following prompt. Of course, I verified the output:\n\nConvert this Python code to a pretty equation I can paste onto Stack Overflow:\n    val = (\n        2 * A_i_minus_2\n        + 3 * A_i_minus_1\n        + 4 * A\n        + 5 * A_i_plus_1\n        # Python ternary operator; don't forget parentheses around the entire ternary expression!\n        + ((6 * B) if B > 0 else (60 * B))\n        + 7 * C\n        - 8 * D\n    )\n\n\n\n\nSee also\n\nThis answer is also posted on my personal website here: https://gabrielstaples.com/python_iterate_over_pandas_dataframe/\n\nhttps://en.wikipedia.org/wiki/Array_programming - array programming, or \"vectorization\":\n\nIn computer science, array programming refers to solutions that allow the application of operations to an entire set of values at once. Such solutions are commonly used in scientific and engineering settings.\nModern programming languages that support array programming (also known as vector or multidimensional languages) have been engineered specifically to generalize operations on scalars to apply transparently to vectors, matrices, and higher-dimensional arrays. These include APL, J, Fortran, MATLAB, Analytica, Octave, R, Cilk Plus, Julia, Perl Data Language (PDL). In these languages, an operation that operates on entire arrays can be called a vectorized operation,1 regardless of whether it is executed on a vector processor, which implements vector instructions.\n\n\nAre for-loops in pandas really bad? When should I care?\n\nmy answer\n\n\nDoes pandas iterrows have performance issues?\n\nThis answer\n\nMy comment underneath it:\n\n...Based on my results, I'd say, however, these are the best approaches, in this order of best first:\n\nvectorization,\nlist comprehension,\n.itertuples(),\n.apply(),\nraw for loop,\n.iterrows().\n\nI didn't test Cython.", "answer_comment": [], "answer_score": "40", "answer_code_list": ["df =\n           A    B    C    D\n0       -365  842  284 -942\n1        532  416 -102  888\n2        397  321 -296 -616\n3       -215  879  557  895\n4        857  701 -157  480\n...      ...  ...  ...  ...\n1999995 -101 -233 -377 -939\n1999996 -989  380  917  145\n1999997 -879  333 -372 -970\n1999998  738  982 -743  312\n1999999 -306 -103  459  745\n", "import numpy as np\nimport pandas as pd\n\n# Create an array (numpy list of lists) of fake data\nMIN_VAL = -1000\nMAX_VAL = 1000\n# NUM_ROWS = 10_000_000\nNUM_ROWS = 2_000_000  # default for final tests\n# NUM_ROWS = 1_000_000\n# NUM_ROWS = 100_000\n# NUM_ROWS = 10_000  # default for rapid development & initial tests\nNUM_COLS = 4\ndata = np.random.randint(MIN_VAL, MAX_VAL, size=(NUM_ROWS, NUM_COLS))\n\n# Now convert it to a Pandas DataFrame with columns named \"A\", \"B\", \"C\", and \"D\"\ndf_original = pd.DataFrame(data, columns=[\"A\", \"B\", \"C\", \"D\"])\nprint(f\"df = \\n{df_original}\")\n", "# Calculate and return a new value, `val`, by performing the following equation:\nval = (\n    2 * A_i_minus_2\n    + 3 * A_i_minus_1\n    + 4 * A\n    + 5 * A_i_plus_1\n    # Python ternary operator; don't forget parentheses around the entire\n    # ternary expression!\n    + ((6 * B) if B > 0 else (60 * B))\n    + 7 * C\n    - 8 * D\n)\n", "# Calculate and return a new value, `val`, by performing the following equation:\n\nif B > 0:\n    B_new = 6 * B\nelse:\n    B_new = 60 * B\n\nval = (\n    2 * A_i_minus_2\n    + 3 * A_i_minus_1\n    + 4 * A\n    + 5 * A_i_plus_1\n    + B_new\n    + 7 * C\n    - 8 * D\n)\n", "def calculate_val(\n        A_i_minus_2,\n        A_i_minus_1,\n        A,\n        A_i_plus_1,\n        B,\n        C,\n        D):\n    val = (\n        2 * A_i_minus_2\n        + 3 * A_i_minus_1\n        + 4 * A\n        + 5 * A_i_plus_1\n        # Python ternary operator; don't forget parentheses around the\n        # entire ternary expression!\n        + ((6 * B) if B > 0 else (60 * B))\n        + 7 * C\n        - 8 * D\n    )\n    return val\n", "val = [np.NAN]*len(df)\nfor i in range(len(df)):\n    if i < 2 or i > len(df)-2:\n        continue\n\n    val[i] = calculate_val(\n        df[\"A\"][i-2],\n        df[\"A\"][i-1],\n        df[\"A\"][i],\n        df[\"A\"][i+1],\n        df[\"B\"][i],\n        df[\"C\"][i],\n        df[\"D\"][i],\n    )\ndf[\"val\"] = val  # put this column back into the dataframe\n", "val = [np.NAN]*len(df)\nfor i in range(len(df)):\n    if i < 2 or i > len(df)-2:\n        continue\n\n    val[i] = calculate_val(\n        df.loc[i-2, \"A\"],\n        df.loc[i-1, \"A\"],\n        df.loc[i,   \"A\"],\n        df.loc[i+1, \"A\"],\n        df.loc[i,   \"B\"],\n        df.loc[i,   \"C\"],\n        df.loc[i,   \"D\"],\n    )\n\ndf[\"val\"] = val  # put this column back into the dataframe\n", "# column indices\ni_A = 0\ni_B = 1\ni_C = 2\ni_D = 3\n\nval = [np.NAN]*len(df)\nfor i in range(len(df)):\n    if i < 2 or i > len(df)-2:\n        continue\n\n    val[i] = calculate_val(\n        df.iloc[i-2, i_A],\n        df.iloc[i-1, i_A],\n        df.iloc[i,   i_A],\n        df.iloc[i+1, i_A],\n        df.iloc[i,   i_B],\n        df.iloc[i,   i_C],\n        df.iloc[i,   i_D],\n    )\n\ndf[\"val\"] = val  # put this column back into the dataframe\n", "val = [np.NAN]*len(df)\nfor index, row in df.iterrows():\n    if index < 2 or index > len(df)-2:\n        continue\n\n    val[index] = calculate_val(\n        df[\"A\"][index-2],\n        df[\"A\"][index-1],\n        row[\"A\"],\n        df[\"A\"][index+1],\n        row[\"B\"],\n        row[\"C\"],\n        row[\"D\"],\n    )\n\ndf[\"val\"] = val  # put this column back into the dataframe\n", "df_original[\"A_i_minus_2\"] = df_original[\"A\"].shift(2)  # val at index i-2\ndf_original[\"A_i_minus_1\"] = df_original[\"A\"].shift(1)  # val at index i-1\ndf_original[\"A_i_plus_1\"] = df_original[\"A\"].shift(-1)  # val at index i+1\n\n# Note: to ensure that no partial calculations are ever done with rows which\n# have NaN values due to the shifting, we can either drop such rows with\n# `.dropna()`, or set all values in these rows to NaN. I'll choose the latter\n# so that the stats that will be generated with the techniques below will end\n# up matching the stats which were produced by the prior techniques above. ie:\n# the number of rows will be identical to before.\n#\n# df_original = df_original.dropna()\ndf_original.iloc[:2, :] = np.NAN   # slicing operators: first two rows,\n                                   # all columns\ndf_original.iloc[-1:, :] = np.NAN  # slicing operators: last row, all columns\n", "val = [np.NAN]*len(df)\nfor row in df.itertuples():\n    val[row.Index] = calculate_val(\n        row.A_i_minus_2,\n        row.A_i_minus_1,\n        row.A,\n        row.A_i_plus_1,\n        row.B,\n        row.C,\n        row.D,\n    )\n\ndf[\"val\"] = val  # put this column back into the dataframe\n", "def calculate_new_column_b_value(b_value):\n    # Python ternary operator\n    b_value_new = (6 * b_value) if b_value > 0 else (60 * b_value)\n    return b_value_new\n\n# In this particular example, since we have an embedded `if-else` statement\n# for the `B` column, pure vectorization is less intuitive. So, first we'll\n# calculate a new `B` column using\n# **`apply()`**, then we'll use vectorization for the rest.\ndf[\"B_new\"] = df[\"B\"].apply(calculate_new_column_b_value)\n# OR (same thing, but with a lambda function instead)\n# df[\"B_new\"] = df[\"B\"].apply(lambda x: (6 * x) if x > 0 else (60 * x))\n\n# Now we can use vectorization for the rest. \"Vectorization\" in this case\n# means to simply use the column series variables in equations directly,\n# without manually iterating over them. Pandas DataFrames will handle the\n# underlying iteration automatically for you. You just focus on the math.\ndf[\"val\"] = (\n    2 * df[\"A_i_minus_2\"]\n    + 3 * df[\"A_i_minus_1\"]\n    + 4 * df[\"A\"]\n    + 5 * df[\"A_i_plus_1\"]\n    + df[\"B_new\"]\n    + 7 * df[\"C\"]\n    - 8 * df[\"D\"]\n)\n", "# In this particular example, since we have an embedded `if-else` statement\n# for the `B` column, pure vectorization is less intuitive. So, first we'll\n# calculate a new `B` column using **list comprehension**, then we'll use\n# vectorization for the rest.\ndf[\"B_new\"] = [\n    calculate_new_column_b_value(b_value) for b_value in df[\"B\"]\n]\n\n# Now we can use vectorization for the rest. \"Vectorization\" in this case\n# means to simply use the column series variables in equations directly,\n# without manually iterating over them. Pandas DataFrames will handle the\n# underlying iteration automatically for you. You just focus on the math.\ndf[\"val\"] = (\n    2 * df[\"A_i_minus_2\"]\n    + 3 * df[\"A_i_minus_1\"]\n    + 4 * df[\"A\"]\n    + 5 * df[\"A_i_plus_1\"]\n    + df[\"B_new\"]\n    + 7 * df[\"C\"]\n    - 8 * df[\"D\"]\n)\n", "# If statement to evaluate:\n#\n#     if B > 0:\n#         B_new = 6 * B\n#     else:\n#         B_new = 60 * B\n#\n# In this particular example, since we have an embedded `if-else` statement\n# for the `B` column, we can use some boolean array indexing through\n# `df.loc[]` for some pure vectorization magic.\n#\n# Explanation:\n#\n# Long:\n#\n# The format is: `df.loc[rows, columns]`, except in this case, the rows are\n# specified by a \"boolean array\" (AKA: a boolean expression, list of\n# booleans, or \"boolean mask\"), specifying all rows where `B` is > 0. Then,\n# only in that `B` column for those rows, set the value accordingly. After\n# we do this for where `B` is > 0, we do the same thing for where `B`\n# is <= 0, except with the other equation.\n#\n# Short:\n#\n# For all rows where the boolean expression applies, set the column value\n# accordingly.\n#\n# GitHub CoPilot first showed me this `.loc[]` technique.\n# See also the official documentation:\n# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html\n#\n# ===========================\n# 1st: handle the > 0 case\n# ===========================\ndf[\"B_new\"] = df.loc[df[\"B\"] > 0, \"B\"] * 6\n#\n# ===========================\n# 2nd: handle the <= 0 case, merging the results into the\n# previously-created \"B_new\" column\n# ===========================\n# - NB: this does NOT work; it overwrites and replaces the whole \"B_new\"\n#   column instead:\n#\n#       df[\"B_new\"] = df.loc[df[\"B\"] <= 0, \"B\"] * 60\n#\n# This works:\ndf.loc[df[\"B\"] <= 0, \"B_new\"] = df.loc[df[\"B\"] <= 0, \"B\"] * 60\n\n# Now use normal vectorization for the rest.\ndf[\"val\"] = (\n    2 * df[\"A_i_minus_2\"]\n    + 3 * df[\"A_i_minus_1\"]\n    + 4 * df[\"A\"]\n    + 5 * df[\"A_i_plus_1\"]\n    + df[\"B_new\"]\n    + 7 * df[\"C\"]\n    - 8 * df[\"D\"]\n)\n", "df[\"val\"] = df.apply(\n    lambda row: calculate_val(\n        row[\"A_i_minus_2\"],\n        row[\"A_i_minus_1\"],\n        row[\"A\"],\n        row[\"A_i_plus_1\"],\n        row[\"B\"],\n        row[\"C\"],\n        row[\"D\"]\n    ),\n    axis='columns' # same as `axis=1`: \"apply function to each row\",\n                   # rather than to each column\n)\n", "df[\"val\"] = [\n    # Note: you *could* do the calculations directly here instead of using a\n    # function call, so long as you don't have indented code blocks such as\n    # sub-routines or multi-line if statements.\n    #\n    # I'm using a function call.\n    calculate_val(\n        A_i_minus_2,\n        A_i_minus_1,\n        A,\n        A_i_plus_1,\n        B,\n        C,\n        D\n    ) for A_i_minus_2, A_i_minus_1, A, A_i_plus_1, B, C, D\n    in zip(\n        df[\"A_i_minus_2\"],\n        df[\"A_i_minus_1\"],\n        df[\"A\"],\n        df[\"A_i_plus_1\"],\n        df[\"B\"],\n        df[\"C\"],\n        df[\"D\"]\n    )\n]\n", "df[\"val\"] = [\n    2 * A_i_minus_2\n    + 3 * A_i_minus_1\n    + 4 * A\n    + 5 * A_i_plus_1\n    # Python ternary operator; don't forget parentheses around the entire\n    # ternary expression!\n    + ((6 * B) if B > 0 else (60 * B))\n    + 7 * C\n    - 8 * D\n    for A_i_minus_2, A_i_minus_1, A, A_i_plus_1, B, C, D\n    in zip(\n        df[\"A_i_minus_2\"],\n        df[\"A_i_minus_1\"],\n        df[\"A\"],\n        df[\"A_i_plus_1\"],\n        df[\"B\"],\n        df[\"C\"],\n        df[\"D\"]\n    )\n]\n", "df[\"val\"] = [\n    calculate_val(\n        row[0],\n        row[1],\n        row[2],\n        row[3],\n        row[4],\n        row[5],\n        row[6],\n    ) for row\n    in zip(\n        df[\"A_i_minus_2\"],\n        df[\"A_i_minus_1\"],\n        df[\"A\"],\n        df[\"A_i_plus_1\"],\n        df[\"B\"],\n        df[\"C\"],\n        df[\"D\"]\n    )\n]\n", "df[\"val\"] = [\n    # Note: you *could* do the calculations directly here instead of using a\n    # function call, so long as you don't have indented code blocks such as\n    # sub-routines or multi-line if statements.\n    #\n    # I'm using a function call.\n    calculate_val(\n        A_i_minus_2,\n        A_i_minus_1,\n        A,\n        A_i_plus_1,\n        B,\n        C,\n        D\n    ) for A_i_minus_2, A_i_minus_1, A, A_i_plus_1, B, C, D\n        # Note: this `[[...]]` double-bracket indexing is used to select a\n        # subset of columns from the dataframe. The inner `[]` brackets\n        # create a list from the column names within them, and the outer\n        # `[]` brackets accept this list to index into the dataframe and\n        # select just this list of columns, in that order.\n        # - See the official documentation on it here:\n        #   https://pandas.pydata.org/docs/user_guide/indexing.html#basics\n        #   - Search for the phrase \"You can pass a list of columns to [] to\n        #     select columns in that order.\"\n        #   - I learned this from this comment here:\n        #     https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas/55557758#comment136020567_55557758\n        # - One of the **list comprehension** examples in this answer here\n        #   uses `.to_numpy()` like this:\n        #   https://stackoverflow.com/a/55557758/4561887\n    in df[[\n        \"A_i_minus_2\",\n        \"A_i_minus_1\",\n        \"A\",\n        \"A_i_plus_1\",\n        \"B\",\n        \"C\",\n        \"D\"\n    ]].to_numpy()  # NB: `.values` works here too, but is deprecated. See:\n                   # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.values.html\n]\n", "if i < 2 or i > len(df)-2:\n    continue\n", "    val = (\n        2 * A_i_minus_2\n        + 3 * A_i_minus_1\n        + 4 * A\n        + 5 * A_i_plus_1\n        # Python ternary operator; don't forget parentheses around the entire ternary expression!\n        + ((6 * B) if B > 0 else (60 * B))\n        + 7 * C\n        - 8 * D\n    )\n"], "is_accepted": false}, {"answer_content": "You can write your own iterator that implements namedtuple\nfrom collections import namedtuple\n\ndef myiter(d, cols=None):\n    if cols is None:\n        v = d.values.tolist()\n        cols = d.columns.values.tolist()\n    else:\n        j = [d.columns.get_loc(c) for c in cols]\n        v = d.values[:, j].tolist()\n\n    n = namedtuple('MyTuple', cols)\n\n    for line in iter(v):\n        yield n(*line)\n\nThis is directly comparable to pd.DataFrame.itertuples.  I'm aiming at performing the same task with more efficiency.\n\nFor the given dataframe with my function:\nlist(myiter(df))\n\n[MyTuple(c1=10, c2=100), MyTuple(c1=11, c2=110), MyTuple(c1=12, c2=120)]\n\nOr with pd.DataFrame.itertuples:\nlist(df.itertuples(index=False))\n\n[Pandas(c1=10, c2=100), Pandas(c1=11, c2=110), Pandas(c1=12, c2=120)]\n\n\nA comprehensive test\nWe test making all columns available and subsetting the columns.  \ndef iterfullA(d):\n    return list(myiter(d))\n\ndef iterfullB(d):\n    return list(d.itertuples(index=False))\n\ndef itersubA(d):\n    return list(myiter(d, ['col3', 'col4', 'col5', 'col6', 'col7']))\n\ndef itersubB(d):\n    return list(d[['col3', 'col4', 'col5', 'col6', 'col7']].itertuples(index=False))\n\nres = pd.DataFrame(\n    index=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\n    columns='iterfullA iterfullB itersubA itersubB'.split(),\n    dtype=float\n)\n\nfor i in res.index:\n    d = pd.DataFrame(np.random.randint(10, size=(i, 10))).add_prefix('col')\n    for j in res.columns:\n        stmt = '{}(d)'.format(j)\n        setp = 'from __main__ import d, {}'.format(j)\n        res.at[i, j] = timeit(stmt, setp, number=100)\n\nres.groupby(res.columns.str[4:-1], axis=1).plot(loglog=True);", "answer_comment": ["For people who don't want to read the code: blue line is intertuples, orange line is a list of an iterator thru a yield block. interrows is not compared."], "answer_score": "32", "answer_code_list": ["from collections import namedtuple\n\ndef myiter(d, cols=None):\n    if cols is None:\n        v = d.values.tolist()\n        cols = d.columns.values.tolist()\n    else:\n        j = [d.columns.get_loc(c) for c in cols]\n        v = d.values[:, j].tolist()\n\n    n = namedtuple('MyTuple', cols)\n\n    for line in iter(v):\n        yield n(*line)\n", "list(myiter(df))\n\n[MyTuple(c1=10, c2=100), MyTuple(c1=11, c2=110), MyTuple(c1=12, c2=120)]\n", "list(df.itertuples(index=False))\n\n[Pandas(c1=10, c2=100), Pandas(c1=11, c2=110), Pandas(c1=12, c2=120)]\n", "def iterfullA(d):\n    return list(myiter(d))\n\ndef iterfullB(d):\n    return list(d.itertuples(index=False))\n\ndef itersubA(d):\n    return list(myiter(d, ['col3', 'col4', 'col5', 'col6', 'col7']))\n\ndef itersubB(d):\n    return list(d[['col3', 'col4', 'col5', 'col6', 'col7']].itertuples(index=False))\n\nres = pd.DataFrame(\n    index=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\n    columns='iterfullA iterfullB itersubA itersubB'.split(),\n    dtype=float\n)\n\nfor i in res.index:\n    d = pd.DataFrame(np.random.randint(10, size=(i, 10))).add_prefix('col')\n    for j in res.columns:\n        stmt = '{}(d)'.format(j)\n        setp = 'from __main__ import d, {}'.format(j)\n        res.at[i, j] = timeit(stmt, setp, number=100)\n\nres.groupby(res.columns.str[4:-1], axis=1).plot(loglog=True);\n"], "is_accepted": false}, {"answer_content": "To loop all rows in a dataframe you can use:\nfor x in range(len(date_example.index)):\n    print date_example['Date'].iloc[x]", "answer_comment": ["If you want to make this work, call df.columns.get_loc to get the integer index position of the date column (outside the loop), then use a single iloc indexing call inside."], "answer_score": "30", "answer_code_list": ["for x in range(len(date_example.index)):\n    print date_example['Date'].iloc[x]\n"], "is_accepted": false}, {"answer_content": "for ind in df.index:\n     print df['c1'][ind], df['c2'][ind]", "answer_comment": ["This is chained indexing. Do not use this!"], "answer_score": "26", "answer_code_list": [" for ind in df.index:\n     print df['c1'][ind], df['c2'][ind]\n"], "is_accepted": false}, {"answer_content": "Sometimes a useful pattern is:\n# Borrowing @KutalmisB df example\ndf = pd.DataFrame({'col1': [1, 2], 'col2': [0.1, 0.2]}, index=['a', 'b'])\n# The to_dict call results in a list of dicts\n# where each row_dict is a dictionary with k:v pairs of columns:value for that row\nfor row_dict in df.to_dict(orient='records'):\n    print(row_dict)\n\nWhich results in:\n{'col1':1.0, 'col2':0.1}\n{'col1':2.0, 'col2':0.2}", "answer_comment": [], "answer_score": "20", "answer_code_list": ["# Borrowing @KutalmisB df example\ndf = pd.DataFrame({'col1': [1, 2], 'col2': [0.1, 0.2]}, index=['a', 'b'])\n# The to_dict call results in a list of dicts\n# where each row_dict is a dictionary with k:v pairs of columns:value for that row\nfor row_dict in df.to_dict(orient='records'):\n    print(row_dict)\n", "{'col1':1.0, 'col2':0.1}\n{'col1':2.0, 'col2':0.2}\n"], "is_accepted": false}, {"answer_content": "Update: cs95 has updated his answer to include plain numpy vectorization. You can simply refer to his answer.\n\ncs95 shows that Pandas vectorization far outperforms other Pandas methods for computing stuff with dataframes.\nI wanted to add that if you first convert the dataframe to a NumPy array and then use vectorization, it's even faster than Pandas dataframe vectorization, (and that includes the time to turn it back into a dataframe series).\nIf you add the following functions to cs95's benchmark code, this becomes pretty evident:\ndef np_vectorization(df):\n    np_arr = df.to_numpy()\n    return pd.Series(np_arr[:,0] + np_arr[:,1], index=df.index)\n\ndef just_np_vectorization(df):\n    np_arr = df.to_numpy()\n    return np_arr[:,0] + np_arr[:,1]", "answer_comment": ["cs95's benchmarking code, for your reference"], "answer_score": "19", "answer_code_list": ["def np_vectorization(df):\n    np_arr = df.to_numpy()\n    return pd.Series(np_arr[:,0] + np_arr[:,1], index=df.index)\n\ndef just_np_vectorization(df):\n    np_arr = df.to_numpy()\n    return np_arr[:,0] + np_arr[:,1]\n"], "is_accepted": false}, {"answer_content": "In short\n\nUse vectorization if possible\nIf an operation can't be vectorized - use list comprehensions\nIf you need a single object representing the entire row - use itertuples\nIf the above is too slow - try swifter.apply\nIf it's still too slow - try a Cython routine\n\nBenchmark", "answer_comment": ["Cython will help ofc but numpy/numba probably more accessible for most people"], "answer_score": "18", "answer_code_list": [], "is_accepted": false}, {"answer_content": "To loop all rows in a dataframe and use values of each row conveniently, namedtuples can be converted to ndarrays. For example:\ndf = pd.DataFrame({'col1': [1, 2], 'col2': [0.1, 0.2]}, index=['a', 'b'])\n\nIterating over the rows:\nfor row in df.itertuples(index=False, name='Pandas'):\n    print np.asarray(row)\n\nresults in:\n[ 1.   0.1]\n[ 2.   0.2]\n\nPlease note that if index=True, the index is added as the first element of the tuple, which may be undesirable for some applications.", "answer_comment": [], "answer_score": "17", "answer_code_list": ["df = pd.DataFrame({'col1': [1, 2], 'col2': [0.1, 0.2]}, index=['a', 'b'])\n", "for row in df.itertuples(index=False, name='Pandas'):\n    print np.asarray(row)\n", "[ 1.   0.1]\n[ 2.   0.2]\n"], "is_accepted": false}, {"answer_content": "There is a way to iterate throw rows while getting a DataFrame in return, and not a Series. I don't see anyone mentioning that you can pass index as a list for the row to be returned as a DataFrame:\nfor i in range(len(df)):\n    row = df.iloc[[i]]\n\nNote the usage of double brackets. This returns a DataFrame with a single row.", "answer_comment": [], "answer_score": "17", "answer_code_list": ["for i in range(len(df)):\n    row = df.iloc[[i]]\n"], "is_accepted": false}, {"answer_content": "I recommend using df.at[row, column] (source) for iterate all pandas cells.\nFor example:\nfor row in range(len(df)):\n  print(df.at[row, 'c1'], df.at[row, 'c2'])\n\nThe output will be:\n10 100\n11 110\n12 120\n\n\nBonus\nYou can also modify the value of cells with df.at[row, column] = newValue.\nfor row in range(len(df)):\n  df.at[row, 'c1'] = 'data-' + str(df.at[row, 'c1'])\n  print(df.at[row, 'c1'], df.at[row, 'c2']) \n\nThe output will be:\ndata-10 100\ndata-11 110\ndata-12 120", "answer_comment": [], "answer_score": "17", "answer_code_list": ["for row in range(len(df)):\n  print(df.at[row, 'c1'], df.at[row, 'c2'])\n", "10 100\n11 110\n12 120\n", "for row in range(len(df)):\n  df.at[row, 'c1'] = 'data-' + str(df.at[row, 'c1'])\n  print(df.at[row, 'c1'], df.at[row, 'c2']) \n", "data-10 100\ndata-11 110\ndata-12 120\n"], "is_accepted": false}, {"answer_content": "Sometimes loops really are better than vectorized code\nAs many answers here correctly point out, your default plan in Pandas should be to write vectorized code (with its implicit loops) rather than attempting an explicit loop yourself.  But the question remains whether you should ever write loops in Pandas, and if so what's the best way to loop in those situations.\nI believe there is at least one general situation where loops are appropriate: when you need to calculate some function that depends on values in other rows in a somewhat complex manner.  In this case, the looping code is often simpler, more readable, and less error prone than vectorized code.\nThe looping code might even be faster too, as you'll see below, so loops might make sense in cases where speed is of utmost importance. But really, those are just going to be subsets of cases where you probably should have been working in numpy/numba (rather than Pandas) to begin with, because optimized numpy/numba will almost always be faster than Pandas.\nLet's show this with an example.  Suppose you want to take a cumulative sum of a column, but reset it whenever some other column equals zero:\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame( { 'x':[1,2,3,4,5,6], 'y':[1,1,1,0,1,1]  } )\n\n#   x  y  desired_result\n#0  1  1               1\n#1  2  1               3\n#2  3  1               6\n#3  4  0               4\n#4  5  1               9\n#5  6  1              15\n\nThis is a good example where you could certainly write one line of Pandas to achieve this, although it's not especially readable, especially if you aren't fairly experienced with Pandas already:\ndf.groupby( (df.y==0).cumsum() )['x'].cumsum()\n\nThat's going to be fast enough for most situations, although you could also write faster code by avoiding the groupby, but it will likely be even less readable.\nAlternatively, what if we write this as a loop?  You could do something like the following with NumPy:\nimport numba as nb\n\n@nb.jit(nopython=True)  # Optional\ndef custom_sum(x,y):\n    x_sum = x.copy()\n    for i in range(1,len(df)):\n        if y[i] > 0: x_sum[i] = x_sum[i-1] + x[i]\n    return x_sum\n\ndf['desired_result'] = custom_sum( df.x.to_numpy(), df.y.to_numpy() )\n\nAdmittedly, there's a bit of overhead there required to convert DataFrame columns to NumPy arrays, but the core piece of code is just one line of code that you could read even if you didn't know anything about Pandas or NumPy:\nif y[i] > 0: x_sum[i] = x_sum[i-1] + x[i]\n\nAnd this code is actually faster than the vectorized code.  In some quick tests with 100,000 rows, the above is about 10x faster than the groupby approach.  Note that one key to the speed there is numba, which is optional.  Without the \"@nb.jit\" line, the looping code is actually about 10x slower than the groupby approach.\nClearly this example is simple enough that you would likely prefer the one line of pandas to writing a loop with its associated overhead.  However, there are more complex versions of this problem for which the readability or speed of the NumPy/numba loop approach likely makes sense.", "answer_comment": [], "answer_score": "14", "answer_code_list": ["import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame( { 'x':[1,2,3,4,5,6], 'y':[1,1,1,0,1,1]  } )\n\n#   x  y  desired_result\n#0  1  1               1\n#1  2  1               3\n#2  3  1               6\n#3  4  0               4\n#4  5  1               9\n#5  6  1              15\n", "df.groupby( (df.y==0).cumsum() )['x'].cumsum()\n", "import numba as nb\n\n@nb.jit(nopython=True)  # Optional\ndef custom_sum(x,y):\n    x_sum = x.copy()\n    for i in range(1,len(df)):\n        if y[i] > 0: x_sum[i] = x_sum[i-1] + x[i]\n    return x_sum\n\ndf['desired_result'] = custom_sum( df.x.to_numpy(), df.y.to_numpy() )\n", "if y[i] > 0: x_sum[i] = x_sum[i-1] + x[i]\n"], "is_accepted": false}, {"answer_content": "For both viewing and modifying values, I would use iterrows(). In a for loop and by using tuple unpacking (see the example: i, row), I use the row for only viewing the value and use i with the loc method when I want to modify values. As stated in previous answers, here you should not modify something you are iterating over.\nfor i, row in df.iterrows():\n    df_column_A = df.loc[i, 'A']\n    if df_column_A == 'Old_Value':\n        df_column_A = 'New_value'  \n\nHere the row in the loop is a copy of that row, and not a view of it. Therefore, you should NOT write something like row['A'] = 'New_Value', it will not modify the DataFrame. However, you can use i and loc and specify the DataFrame to do the work.", "answer_comment": [], "answer_score": "13", "answer_code_list": ["for i, row in df.iterrows():\n    df_column_A = df.loc[i, 'A']\n    if df_column_A == 'Old_Value':\n        df_column_A = 'New_value'  \n"], "is_accepted": false}, {"answer_content": "There are so many ways to iterate over the rows in Pandas dataframe. One very simple and intuitive way is:\ndf = pd.DataFrame({'A':[1, 2, 3], 'B':[4, 5, 6], 'C':[7, 8, 9]})\nprint(df)\nfor i in range(df.shape[0]):\n    # For printing the second column\n    print(df.iloc[i, 1])\n\n    # For printing more than one columns\n    print(df.iloc[i, [0, 2]])", "answer_comment": [], "answer_score": "11", "answer_code_list": ["df = pd.DataFrame({'A':[1, 2, 3], 'B':[4, 5, 6], 'C':[7, 8, 9]})\nprint(df)\nfor i in range(df.shape[0]):\n    # For printing the second column\n    print(df.iloc[i, 1])\n\n    # For printing more than one columns\n    print(df.iloc[i, [0, 2]])\n"], "is_accepted": false}, {"answer_content": "The easiest way, use the apply function\ndef print_row(row):\n   print row['c1'], row['c2']\n\ndf.apply(lambda row: print_row(row), axis=1)", "answer_comment": [], "answer_score": "10", "answer_code_list": ["def print_row(row):\n   print row['c1'], row['c2']\n\ndf.apply(lambda row: print_row(row), axis=1)\n"], "is_accepted": false}, {"answer_content": "Probably the most elegant solution (but certainly not the most efficient):\nfor row in df.values:\n    c2 = row[1]\n    print(row)\n    # ...\n\nfor c1, c2 in df.values:\n    # ...\n\nNote that:\n\nthe documentation explicitly recommends to use .to_numpy() instead\nthe produced NumPy array will have a dtype that fits all columns, in the worst case object\nthere are good reasons not to use a loop in the first place\n\nStill, I think this option should be included here, as a straightforward solution to a (one should think) trivial problem.", "answer_comment": [], "answer_score": "9", "answer_code_list": ["for row in df.values:\n    c2 = row[1]\n    print(row)\n    # ...\n\nfor c1, c2 in df.values:\n    # ...\n"], "is_accepted": false}, {"answer_content": "1. Iterate over df.index and access via at[]\nA method that is quite readable is to iterate over the index (as suggested by @Grag2015). However, instead of chained indexing used there, use at for efficiency:\nfor ind in df.index:\n    print(df.at[ind, 'col A'])\n\nThe advantage of this method over for i in range(len(df)) is it works even if the index is not RangeIndex. See the following example:\ndf = pd.DataFrame({'col A': list('ABCDE'), 'col B': range(5)}, index=list('abcde'))\n\nfor ind in df.index:\n    print(df.at[ind, 'col A'], df.at[ind, 'col B'])    # <---- OK\n    df.at[ind, 'col C'] = df.at[ind, 'col B'] * 2      # <---- can assign values\n        \nfor ind in range(len(df)):\n    print(df.at[ind, 'col A'], df.at[ind, 'col B'])    # <---- KeyError\n\nIf the integer location of a row is needed (e.g. to get previous row's values), wrap it by enumerate():\nfor i, ind in enumerate(df.index):\n    prev_row_ind = df.index[i-1] if i > 0 else df.index[i]\n    df.at[ind, 'col C'] = df.at[prev_row_ind, 'col B'] * 2\n\n\n2. Use get_loc with itertuples()\nEven though it's much faster than iterrows(), a major drawback of itertuples() is that it mangles column labels if they contain space in them (e.g. 'col C' becomes _1 etc.), which makes it hard to access values in iteration.\nYou can use df.columns.get_loc() to get the integer location of a column label and use it to index the namedtuples. Note that the first element of each namedtuple is the index label, so to properly access the column by integer position, you either have to add 1 to whatever is returned from get_loc or unpack the tuple in the beginning.\ndf = pd.DataFrame({'col A': list('ABCDE'), 'col B': range(5)}, index=list('abcde'))\n\nfor row in df.itertuples(name=None):\n    pos = df.columns.get_loc('col B') + 1              # <---- add 1 here\n    print(row[pos])\n\n\nfor ind, *row in df.itertuples(name=None):\n#   ^^^^^^^^^    <---- unpacked here\n    pos = df.columns.get_loc('col B')                  # <---- already unpacked\n    df.at[ind, 'col C'] = row[pos] * 2\n    print(row[pos])\n\n\n3. Convert to a dictionary and iterate over dict_items\nAnother way to loop over a dataframe is to convert it into a dictionary in orient='index' and iterate over the dict_items or dict_values.\ndf = pd.DataFrame({'col A': list('ABCDE'), 'col B': range(5)})\n\nfor row in df.to_dict('index').values():\n#                             ^^^^^^^^^         <--- iterate over dict_values\n    print(row['col A'], row['col B'])\n\n\nfor index, row in df.to_dict('index').items():\n#                                    ^^^^^^^^   <--- iterate over dict_items\n    df.at[index, 'col A'] = row['col A'] + str(row['col B'])\n\nThis doesn't mangle dtypes like iterrows, doesn't mangle column labels like itertuples and agnostic about the number of columns (zip(df['col A'], df['col B'], ...) would quickly turn cumbersome if there are many columns).\n\nFinally, as @cs95 mentioned, avoid looping as much possible. Especially if your data is numeric, there'll be an optimized method for your task in the library if you dig a little bit.\nThat said, there are some cases where iteration is more efficient than vectorized operations. One common such task is to dump a pandas dataframe into a nested json. At least as of pandas 1.5.3, an itertuples() loop is much faster than any vectorized operation involving groupby.apply method in that case.", "answer_comment": [], "answer_score": "8", "answer_code_list": ["for ind in df.index:\n    print(df.at[ind, 'col A'])\n", "df = pd.DataFrame({'col A': list('ABCDE'), 'col B': range(5)}, index=list('abcde'))\n\nfor ind in df.index:\n    print(df.at[ind, 'col A'], df.at[ind, 'col B'])    # <---- OK\n    df.at[ind, 'col C'] = df.at[ind, 'col B'] * 2      # <---- can assign values\n        \nfor ind in range(len(df)):\n    print(df.at[ind, 'col A'], df.at[ind, 'col B'])    # <---- KeyError\n", "for i, ind in enumerate(df.index):\n    prev_row_ind = df.index[i-1] if i > 0 else df.index[i]\n    df.at[ind, 'col C'] = df.at[prev_row_ind, 'col B'] * 2\n", "df = pd.DataFrame({'col A': list('ABCDE'), 'col B': range(5)}, index=list('abcde'))\n\nfor row in df.itertuples(name=None):\n    pos = df.columns.get_loc('col B') + 1              # <---- add 1 here\n    print(row[pos])\n\n\nfor ind, *row in df.itertuples(name=None):\n#   ^^^^^^^^^    <---- unpacked here\n    pos = df.columns.get_loc('col B')                  # <---- already unpacked\n    df.at[ind, 'col C'] = row[pos] * 2\n    print(row[pos])\n", "df = pd.DataFrame({'col A': list('ABCDE'), 'col B': range(5)})\n\nfor row in df.to_dict('index').values():\n#                             ^^^^^^^^^         <--- iterate over dict_values\n    print(row['col A'], row['col B'])\n\n\nfor index, row in df.to_dict('index').items():\n#                                    ^^^^^^^^   <--- iterate over dict_items\n    df.at[index, 'col A'] = row['col A'] + str(row['col B'])\n"], "is_accepted": false}, {"answer_content": "You can also do NumPy indexing for even greater speed ups. It's not really iterating but works much better than iteration for certain applications.\nsubset = row['c1'][0:5]\nall = row['c1'][:]\n\nYou may also want to cast it to an array. These indexes/selections are supposed to act like NumPy arrays already, but I ran into issues and needed to cast\nnp.asarray(all)\nimgs[:] = cv2.resize(imgs[:], (224,224) ) # Resize every image in an hdf5 file", "answer_comment": [], "answer_score": "7", "answer_code_list": ["subset = row['c1'][0:5]\nall = row['c1'][:]\n", "np.asarray(all)\nimgs[:] = cv2.resize(imgs[:], (224,224) ) # Resize every image in an hdf5 file\n"], "is_accepted": false}, {"answer_content": "df.iterrows() returns tuple(a, b) where a is the index and b is the row.", "answer_comment": [], "answer_score": "6", "answer_code_list": [], "is_accepted": false}, {"answer_content": "This example uses iloc to isolate each digit in the data frame. \nimport pandas as pd\n\n a = [1, 2, 3, 4]\n b = [5, 6, 7, 8]\n\n mjr = pd.DataFrame({'a':a, 'b':b})\n\n size = mjr.shape\n\n for i in range(size[0]):\n     for j in range(size[1]):\n         print(mjr.iloc[i, j])", "answer_comment": [], "answer_score": "5", "answer_code_list": ["import pandas as pd\n\n a = [1, 2, 3, 4]\n b = [5, 6, 7, 8]\n\n mjr = pd.DataFrame({'a':a, 'b':b})\n\n size = mjr.shape\n\n for i in range(size[0]):\n     for j in range(size[1]):\n         print(mjr.iloc[i, j])\n"], "is_accepted": false}, {"answer_content": "Disclaimer: Although here are so many answers which recommend not using an iterative (loop) approach (and I mostly agree), I would still see it as a reasonable approach for the following situation:\nExtend a dataframe with data from an API\nLet's say you have a large dataframe which contains incomplete user data. Now you have to extend this data with additional columns, for example, the user's age and gender.\nBoth values have to be fetched from a backend API. I'm assuming the API doesn't provide a \"batch\" endpoint (which would accept multiple user IDs at once). Otherwise, you should rather call the API only once.\nThe costs (waiting time) for the network request surpass the iteration of the dataframe by far. We're talking about network round trip times of hundreds of milliseconds compared to the negligibly small gains in using alternative approaches to iterations.\nOne expensive network request for each row\nSo in this case, I would absolutely prefer using an iterative approach. Although the network request is expensive, it is guaranteed being triggered only once for each row in the dataframe. Here is an example using DataFrame.iterrows:\nExample\nfor index, row in users_df.iterrows():\n  user_id = row['user_id']\n\n  # Trigger expensive network request once for each row\n  response_dict = backend_api.get(f'/api/user-data/{user_id}')\n\n  # Extend dataframe with multiple data from response\n  users_df.at[index, 'age'] = response_dict.get('age')\n  users_df.at[index, 'gender'] = response_dict.get('gender')", "answer_comment": [], "answer_score": "4", "answer_code_list": ["for index, row in users_df.iterrows():\n  user_id = row['user_id']\n\n  # Trigger expensive network request once for each row\n  response_dict = backend_api.get(f'/api/user-data/{user_id}')\n\n  # Extend dataframe with multiple data from response\n  users_df.at[index, 'age'] = response_dict.get('age')\n  users_df.at[index, 'gender'] = response_dict.get('gender')\n"], "is_accepted": false}], "views": "7.7m", "title": "How can I iterate over rows in a Pandas DataFrame?", "question_link": "https://stackoverflow.com/questions/16476924/how-can-i-iterate-over-rows-in-a-pandas-dataframe", "question_content": "I have a pandas dataframe, df:\n   c1   c2\n0  10  100\n1  11  110\n2  12  120\n\nHow do I iterate over the rows of this dataframe? For every row, I want to access its elements (values in cells) by the name of the columns. For example:\nfor row in df.rows:\n    print(row['c1'], row['c2'])\n\n\nI found a similar question, which suggests using either of these:\n\n\nfor date, row in df.T.iteritems():\n\n\n\nfor row in df.iterrows():\n\n\n\nBut I do not understand what the row object is and how I can work with it.", "question_comment": ["In contrast to what cs95 says, there are perfectly fine reasons to want to iterate over a dataframe, so new users should not feel discouraged. One example is if you want to execute some code using the values of each row as input. Also, if your dataframe is reasonably small (e.g. less than 1000 items), performance is not really an issue.", "If you are a beginner to this thread and are not familiar with the pandas library, it's worth taking a step back and evaluating whether iteration is indeed the solution to your problem. In some cases, it is. In most cases, it isn't. It is important to introduce beginners to the library by easing them into the concept of vectorization so they know the difference between writing \"good code\", versus \"code that just works\" - and also know when to use which.", "Related: Are for-loops in pandas really bad? When should I care?", "See also: What is the most efficient way to loop through dataframes with pandas?."]}]