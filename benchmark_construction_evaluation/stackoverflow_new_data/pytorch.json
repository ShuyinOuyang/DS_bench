[{"votes": "621", "answers": [{"answer_content": "These functions should help:\n>>> import torch\n\n>>> torch.cuda.is_available()\nTrue\n\n>>> torch.cuda.device_count()\n1\n\n>>> torch.cuda.current_device()\n0\n\n>>> torch.cuda.device(0)\n<torch.cuda.device at 0x7efce0b03be0>\n\n>>> torch.cuda.get_device_name(0)\n'GeForce GTX 950M'\n\nThis tells us:\n\nCUDA is available and can be used by one device.\nDevice 0 refers to the GPU GeForce GTX 950M, and it is currently chosen by PyTorch.", "answer_comment": ["I think this just shows that these devices are available on the machine but I'm not sure whether you can get how much memory is being used from each GPU or so..", "running torch.cuda.current_device() was helpful for me. It showed that my gpu is unfortunately too old: \"Found GPU0 GeForce GTX 760 which is of cuda capability 3.0.     PyTorch no longer supports this GPU because it is too old.\"", "torch.cuda.is_available()", "@kmario23 Thanks for pointing this out.  Is there a function call that gives us that information (how much memory is being used by each GPU) ?   :)", "@frank Yep, simply this command: $ watch -n 2 nvidia-smi does the job. For more details, please see my answer below."], "answer_score": "1008", "answer_code_list": [">>> import torch\n\n>>> torch.cuda.is_available()\nTrue\n\n>>> torch.cuda.device_count()\n1\n\n>>> torch.cuda.current_device()\n0\n\n>>> torch.cuda.device(0)\n<torch.cuda.device at 0x7efce0b03be0>\n\n>>> torch.cuda.get_device_name(0)\n'GeForce GTX 950M'\n"], "is_accepted": true}, {"answer_content": "As it hasn't been proposed here, I'm adding a method using torch.device, as this is quite handy, also when initializing tensors on the correct device.\n# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n\nEdit: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved. So use memory_cached for older versions.\nOutput:\nUsing device: cuda\n\nTesla K80\nMemory Usage:\nAllocated: 0.3 GB\nCached:    0.6 GB\n\nAs mentioned above, using device it is possible to:\n\nTo move tensors to the respective device:\ntorch.rand(10).to(device)\n\n\nTo create a tensor directly on the device:\ntorch.rand(10, device=device)\n\n\n\nWhich makes switching between CPU and GPU comfortable without changing the actual code.\n\nEdit:\nAs there has been some questions and confusion about the cached and allocated memory I'm adding some additional information about it:\n\ntorch.cuda.max_memory_cached(device=None) Returns the maximum GPU memory managed by the caching allocator in bytes for a\ngiven device.\n\ntorch.cuda.memory_allocated(device=None) Returns the current GPU memory usage by tensors in bytes for a given device.\n\n\nYou can either directly hand over a device as specified further above in the post or you can leave it None and it will use the current_device().\n\nAdditional note: Old graphic cards with Cuda compute capability 3.0 or lower may be visible but cannot be used by Pytorch! Thanks to hekimgil for pointing this out! - \"Found GPU0 GeForce GT 750M which is of cuda capability 3.0. PyTorch no longer supports this GPU because it is too old. The minimum cuda capability that we support is 3.5.\"", "answer_comment": ["I tried your code, it recognizes the graphics card but the allocated and cached are both 0GB. Is it normal or do I need to configure them?", "@KubiK888 If you haven't done any computation before this is perfectly normal. It's also rather unlikely that you can detect the GPU model within PyTorch but not access it. Try doing some computations on GPU and you should see that the values change.", "@KubiK888 You have to be consistent, you cannot perform operations across devices. Any operation like my_tensor_on_gpu * my_tensor_on_cpu will fail.", "Your answer is great but for the first device assignment line, I would like to point out that just because there is a cuda device available, does not mean that we can use it.  For example, I have this in my trusty old computer:  Found GPU0 GeForce GT 750M which is of cuda capability 3.0. PyTorch no longer supports this GPU because it is too old. The minimum cuda capability that we support is 3.5.", "@CharlieParker I haven't tested this, but I believe you can use torch.cuda.device_count() where list(range(torch.cuda.device_count())) should give you a list over all device indices."], "answer_score": "259", "answer_code_list": ["# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n", "Using device: cuda\n\nTesla K80\nMemory Usage:\nAllocated: 0.3 GB\nCached:    0.6 GB\n", "torch.rand(10).to(device)\n", "torch.rand(10, device=device)\n"], "is_accepted": false}, {"answer_content": "After you start running the training loop, if you want to manually watch it from the terminal whether your program is utilizing the GPU resources and to what extent, then you can simply use watch as in:\n$ watch -n 2 nvidia-smi\n\nThis will continuously update the usage stats for every 2 seconds until you press ctrl+c\n\nIf you need more control on more GPU stats you might need, you can use more sophisticated version of nvidia-smi with --query-gpu=.... Below is a simple illustration of this:\n$ watch -n 3 nvidia-smi --query-gpu=index,gpu_name,memory.total,memory.used,memory.free,temperature.gpu,pstate,utilization.gpu,utilization.memory --format=csv\n\nwhich would output the stats something like:\nEvery 3.0s: nvidia-smi --query-gpu=index,gpu_name,memory.total,memory.used,memory.free,temperature.gpu,pstate,utilization.gpu,utilization.memory --format=csv           Sat Apr 11 12:25:09 2020\n\nindex, name, memory.total [MiB], memory.used [MiB], memory.free [MiB], temperature.gpu, pstate, utilization.gpu [%], utilization.memory [%]\n0, GeForce GTX TITAN X, 12212 MiB, 10593 MiB, 1619 MiB, 86, P2, 100 %, 55 %\n1, GeForce GTX TITAN X, 12212 MiB, 11479 MiB, 733 MiB, 84, P2, 93 %, 100 %\n2, GeForce GTX TITAN X, 12212 MiB, 446 MiB, 11766 MiB, 36, P8, 0 %, 0 %\n3, GeForce GTX TITAN X, 12212 MiB, 11 MiB, 12201 MiB, 38, P8, 0 %, 0 %\n\nNote: There should not be any space between the comma separated query names in --query-gpu=.... Else those values will be ignored and no stats are returned.\n\nAlso, you can check whether your installation of PyTorch detects your CUDA installation correctly by doing:\nIn [13]: import  torch\n\nIn [14]: torch.cuda.is_available()\nOut[14]: True\n\nTrue status means that PyTorch is configured correctly and is using the GPU although you have to move/place the tensors with necessary statements in your code.\n\nIf you want to do this inside Python code, then look into this module:\nhttps://github.com/jonsafari/nvidia-ml-py or in pypi here: https://pypi.python.org/pypi/nvidia-ml-py/", "answer_comment": ["Just remember that PyTorch uses a cached GPU memory allocator. You might see low GPU-Utill for nividia-smi even if it's fully used.", "@JakubBielan thanks! could you please provide a reference for more reading on this?", "That watch is useful", "Is this only for linux?", "nvidia-smi has a flag -l for loop seconds, so you don't  have to use watch: nvidia-smi -l 2  Or in milliseconds: nvidia-smi -lms 2000"], "answer_score": "81", "answer_code_list": ["$ watch -n 2 nvidia-smi\n", "$ watch -n 3 nvidia-smi --query-gpu=index,gpu_name,memory.total,memory.used,memory.free,temperature.gpu,pstate,utilization.gpu,utilization.memory --format=csv\n", "Every 3.0s: nvidia-smi --query-gpu=index,gpu_name,memory.total,memory.used,memory.free,temperature.gpu,pstate,utilization.gpu,utilization.memory --format=csv           Sat Apr 11 12:25:09 2020\n\nindex, name, memory.total [MiB], memory.used [MiB], memory.free [MiB], temperature.gpu, pstate, utilization.gpu [%], utilization.memory [%]\n0, GeForce GTX TITAN X, 12212 MiB, 10593 MiB, 1619 MiB, 86, P2, 100 %, 55 %\n1, GeForce GTX TITAN X, 12212 MiB, 11479 MiB, 733 MiB, 84, P2, 93 %, 100 %\n2, GeForce GTX TITAN X, 12212 MiB, 446 MiB, 11766 MiB, 36, P8, 0 %, 0 %\n3, GeForce GTX TITAN X, 12212 MiB, 11 MiB, 12201 MiB, 38, P8, 0 %, 0 %\n", "In [13]: import  torch\n\nIn [14]: torch.cuda.is_available()\nOut[14]: True\n"], "is_accepted": false}, {"answer_content": "From practical standpoint just one minor digression:\nimport torch\ndev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nThis dev now knows if cuda or cpu.\nAnd there is a difference in how you deal with models and with tensors when moving to cuda. It is a bit strange at first.\nimport torch\nimport torch.nn as nn\ndev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nt1 = torch.randn(1,2)\nt2 = torch.randn(1,2).to(dev)\nprint(t1)  # tensor([[-0.2678,  1.9252]])\nprint(t2)  # tensor([[ 0.5117, -3.6247]], device='cuda:0')\nt1.to(dev)\nprint(t1)  # tensor([[-0.2678,  1.9252]])\nprint(t1.is_cuda) # False\nt1 = t1.to(dev)\nprint(t1)  # tensor([[-0.2678,  1.9252]], device='cuda:0')\nprint(t1.is_cuda) # True\n\nclass M(nn.Module):\n    def __init__(self):        \n        super().__init__()        \n        self.l1 = nn.Linear(1,2)\n\n    def forward(self, x):                      \n        x = self.l1(x)\n        return x\nmodel = M()   # not on cuda\nmodel.to(dev) # is on cuda (all parameters)\nprint(next(model.parameters()).is_cuda) # True\n\nThis all is tricky and understanding it once, helps you to deal fast with less debugging.", "answer_comment": ["also you need at the begning import torch.nn as nn"], "answer_score": "55", "answer_code_list": ["import torch\ndev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n", "import torch\nimport torch.nn as nn\ndev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nt1 = torch.randn(1,2)\nt2 = torch.randn(1,2).to(dev)\nprint(t1)  # tensor([[-0.2678,  1.9252]])\nprint(t2)  # tensor([[ 0.5117, -3.6247]], device='cuda:0')\nt1.to(dev)\nprint(t1)  # tensor([[-0.2678,  1.9252]])\nprint(t1.is_cuda) # False\nt1 = t1.to(dev)\nprint(t1)  # tensor([[-0.2678,  1.9252]], device='cuda:0')\nprint(t1.is_cuda) # True\n\nclass M(nn.Module):\n    def __init__(self):        \n        super().__init__()        \n        self.l1 = nn.Linear(1,2)\n\n    def forward(self, x):                      \n        x = self.l1(x)\n        return x\nmodel = M()   # not on cuda\nmodel.to(dev) # is on cuda (all parameters)\nprint(next(model.parameters()).is_cuda) # True\n"], "is_accepted": false}, {"answer_content": "Query\nCommand\n\n\n\n\nDoes PyTorch see any GPUs?\ntorch.cuda.is_available()\n\n\nAre tensors stored on GPU by default?\ntorch.rand(10).device\n\n\nSet default tensor type to CUDA:\ntorch.set_default_tensor_type(torch.cuda.FloatTensor)\n\n\nIs this tensor a GPU tensor?\nmy_tensor.is_cuda\n\n\nIs this model stored on the GPU?\nall(p.is_cuda for p in my_model.parameters())", "answer_comment": ["I didn't know you could set tensors to be on GPU by default. Cool!", "What if there are multiple gpus. Can I pick which one tensors are created on?", "Set default tensor type to CUDA is deprecated and torch.set_default_device('cuda') is used. nowadays"], "answer_score": "45", "answer_code_list": [], "is_accepted": false}, {"answer_content": "From the official site's get started page, you can check if the GPU is available for PyTorch like so:\nimport torch\ntorch.cuda.is_available()\n\nReference: PyTorch | Get Started", "answer_comment": ["this is the most helpful (concise) answer"], "answer_score": "28", "answer_code_list": ["import torch\ntorch.cuda.is_available()\n"], "is_accepted": false}, {"answer_content": "To check if there is a GPU available:\ntorch.cuda.is_available()\n\nIf the above function returns False, \n\nyou either have no GPU, \nor the Nvidia drivers have not been installed so the OS does not see the GPU, \nor the GPU is being hidden by the environmental variable CUDA_VISIBLE_DEVICES.  When the value of CUDA_VISIBLE_DEVICES is -1, then all your devices are being hidden.  You can check that value in code with this line: os.environ['CUDA_VISIBLE_DEVICES']\n\nIf the above function returns True that does not necessarily mean that you are using the GPU. In Pytorch you can allocate tensors to devices when you create them. By default, tensors get allocated to the cpu.  To check where your tensor is allocated do:\n# assuming that 'a' is a tensor created somewhere else\na.device  # returns the device where the tensor is allocated\n\nNote that you cannot operate on tensors allocated in different devices. To see how to allocate a tensor to the GPU, see here: https://pytorch.org/docs/stable/notes/cuda.html", "answer_comment": [], "answer_score": "14", "answer_code_list": ["torch.cuda.is_available()\n", "# assuming that 'a' is a tensor created somewhere else\na.device  # returns the device where the tensor is allocated\n"], "is_accepted": false}, {"answer_content": "Simply from command prompt or Linux environment run the following command. \npython -c 'import torch; print(torch.cuda.is_available())'\n\nThe above should print True\npython -c 'import torch; print(torch.rand(2,3).cuda())'\n\nThis one should print the following:\ntensor([[0.7997, 0.6170, 0.7042], [0.4174, 0.1494, 0.0516]], device='cuda:0')", "answer_comment": [], "answer_score": "13", "answer_code_list": ["python -c 'import torch; print(torch.cuda.is_available())'\n", "python -c 'import torch; print(torch.rand(2,3).cuda())'\n", "tensor([[0.7997, 0.6170, 0.7042], [0.4174, 0.1494, 0.0516]], device='cuda:0')\n"], "is_accepted": false}, {"answer_content": "Almost all answers here reference torch.cuda.is_available(). However, that's only one part of the coin. It tells you whether the GPU (actually CUDA) is available, not whether it's actually being used. In a typical setup, you would set your device with something like this:\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nbut in larger environments (e.g. research) it is also common to give the user more options, so based on input they can disable CUDA, specify CUDA IDs, and so on. In such case, whether or not the GPU is used is not only based on whether it is available or not. After the device has been set to a torch device, you can get its type property to verify whether it's CUDA or not.\nif device.type == 'cuda':\n    # do something", "answer_comment": [], "answer_score": "10", "answer_code_list": ["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n", "if device.type == 'cuda':\n    # do something\n"], "is_accepted": false}, {"answer_content": "For a MacBook M1 system:\nimport torch\nprint(torch.backends.mps.is_available(), torch.backends.mps.is_built())\n\nAnd both should be True.", "answer_comment": ["Note that this also works for at least some older Intel Macbooks. This works on my 2019 Intel macbook with a Radeon Pro 560X 4gb GPU."], "answer_score": "8", "answer_code_list": ["import torch\nprint(torch.backends.mps.is_available(), torch.backends.mps.is_built())\n"], "is_accepted": false}, {"answer_content": "If you are using Linux I suggest to install nvtop\nhttps://github.com/Syllo/nvtop\nYou will get something like this:", "answer_comment": ["How does nvtop tells you that torch is using cuda or not ?"], "answer_score": "6", "answer_code_list": [], "is_accepted": false}, {"answer_content": "import torch\ntorch.cuda.is_available()\n\nworks fine. If you want to monitor the activity during the usage of torch, you can use this Python script (Windows only - but can be adjusted easily):\nimport io\nimport shutil\nimport subprocess\nfrom time import sleep, strftime\nimport pandas as pd\n\nstartupinfo = subprocess.STARTUPINFO()\nstartupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\nstartupinfo.wShowWindow = subprocess.SW_HIDE\ncreationflags = subprocess.CREATE_NO_WINDOW\ninvisibledict = {\n    \"startupinfo\": startupinfo,\n    \"creationflags\": creationflags,\n    \"start_new_session\": True,\n}\npath = shutil.which(\"nvidia-smi.exe\")\n\n\ndef nvidia_log(savepath=None, sleeptime=1):\n    \"\"\"\n        Monitor NVIDIA GPU information and log the data into a pandas DataFrame.\n\n        Parameters:\n            savepath (str, optional): The file path to save the log data as a CSV file.\n                                      If provided, the data will be saved upon KeyboardInterrupt.\n            sleeptime (int, optional): The time interval (in seconds) between each data logging.\n\n        Returns:\n            pandas.DataFrame: A DataFrame containing the logged NVIDIA GPU information with the following columns:\n                - index: GPU index.\n                - name: GPU name.\n                - memory.total [MiB]: Total GPU memory in MiB (Mebibytes).\n                - memory.used [MiB]: Used GPU memory in MiB (Mebibytes).\n                - memory.free [MiB]: Free GPU memory in MiB (Mebibytes).\n                - temperature.gpu: GPU temperature in Celsius.\n                - pstate: GPU performance state.\n                - utilization.gpu [%]: GPU utilization percentage.\n                - utilization.memory [%]: Memory utilization percentage.\n                - timestamp: Timestamp in the format \"YYYY_MM_DD_HH_MM_SS\".\n\n        Description:\n            This function uses the NVIDIA System Management Interface (nvidia-smi) to query GPU information,\n            including memory usage, temperature, performance state, and utilization. The data is collected\n            in real-time and logged into a pandas DataFrame. The logging continues indefinitely until a\n            KeyboardInterrupt (usually triggered by pressing Ctrl + C).\n\n            If the 'savepath' parameter is provided, the collected GPU information will be saved to a CSV\n            file when the monitoring is interrupted by the user (KeyboardInterrupt).\n\n            Note: This function is intended for systems with NVIDIA GPUs on Windows and requires the nvidia-smi.exe\n            executable to be available in the system path.\n\n        Example:\n            # Start monitoring NVIDIA GPU and display the real-time log\n            nvidia_log()\n\n            # Start monitoring NVIDIA GPU and save the log data to a CSV file\n            nvidia_log(savepath=\"gpu_log.csv\")\n\n            # Start monitoring NVIDIA GPU with a custom time interval between logs (e.g., 2 seconds)\n            nvidia_log(sleeptime=2)\n\n              index                            name  memory.total [MiB]  memory.used [MiB]  memory.free [MiB]  temperature.gpu  pstate  utilization.gpu [%]  utilization.memory [%]            timestamp\n    0     0   NVIDIA GeForce RTX 2060 SUPER            8192 MiB           1321 MiB           6697 MiB               45      P8                 16 %                     5 %  2023_07_18_11_52_55\n      index                            name  memory.total [MiB]  memory.used [MiB]  memory.free [MiB]  temperature.gpu  pstate  utilization.gpu [%]  utilization.memory [%]            timestamp\n    1     0   NVIDIA GeForce RTX 2060 SUPER            8192 MiB           1321 MiB           6697 MiB               44      P8                 17 %                     6 %  2023_07_18_11_52_56\n      index                            name  memory.total [MiB]  memory.used [MiB]  memory.free [MiB]  temperature.gpu  pstate  utilization.gpu [%]  utilization.memory [%]            timestamp\n    2     0   NVIDIA GeForce RTX 2060 SUPER            8192 MiB           1321 MiB           6697 MiB               44      P8                  2 %                     4 %  2023_07_18_11_52_57\n      index                            name  memory.total [MiB]  memory.used [MiB]  memory.free [MiB]  temperature.gpu  pstate  utilization.gpu [%]  utilization.memory [%]            timestamp\n    3     0   NVIDIA GeForce RTX 2060 SUPER            8192 MiB           1321 MiB           6697 MiB               44      P8                  4 %                     5 %  2023_07_18_11_52_58\n      index                            name  memory.total [MiB]  memory.used [MiB]  memory.free [MiB]  temperature.gpu  pstate  utilization.gpu [%]  utilization.memory [%]            timestamp\n    4     0   NVIDIA GeForce RTX 2060 SUPER            8192 MiB           1321 MiB           6697 MiB               46      P2                 22 %                     1 %  2023_07_18_11_52_59\n      index                            name  memory.total [MiB]  memory.used [MiB]  memory.free [MiB]  temperature.gpu  pstate  utilization.gpu [%]  utilization.memory [%]            timestamp\n    5     0   NVIDIA GeForce RTX 2060 SUPER            8192 MiB           1320 MiB           6698 MiB               45      P8                  0 %                     0 %  2023_07_18_11_53_00\n      index                            name  memory.total [MiB]  memory.used [MiB]  memory.free [MiB]  temperature.gpu  pstate  utilization.gpu [%]  utilization.memory [%]            timestamp\n    6     0   NVIDIA GeForce RTX 2060 SUPER            8192 MiB           1320 MiB           6698 MiB               45      P8                  2 %                     4 %  2023_07_18_11_53_01\n      index                            name  memory.total [MiB]  memory.used [MiB]  memory.free [MiB]  temperature.gpu  pstate  utilization.gpu [%]  utilization.memory [%]            timestamp\n    7     0   NVIDIA GeForce RTX 2060 SUPER            8192 MiB           1320 MiB           6698 MiB               44      P8                 12 %                     5 %  2023_07_18_11_53_02\n      index                            name  memory.total [MiB]  memory.used [MiB]  memory.free [MiB]  temperature.gpu  pstate  utilization.gpu [%]  utilization.memory [%]            timestamp\n    8     0   NVIDIA GeForce RTX 2060 SUPER            8192 MiB           1320 MiB           6698 MiB               44      P8                  3 %                     4 %  2023_07_18_11_53_03\n    \"\"\"\n    df = pd.DataFrame(\n        columns=[\n            \"index\",\n            \" name\",\n            \" memory.total [MiB]\",\n            \" memory.used [MiB]\",\n            \" memory.free [MiB]\",\n            \" temperature.gpu\",\n            \" pstate\",\n            \" utilization.gpu [%]\",\n            \" utilization.memory [%]\",\n            \"timestamp\",\n        ]\n    )\n    try:\n        while True:\n            p = subprocess.run(\n                [\n                    path,\n                    \"--query-gpu=index,gpu_name,memory.total,memory.used,memory.free,temperature.gpu,pstate,\"\n                    \"utilization.gpu,utilization.memory\",\n                    \"--format=csv\",\n                ],\n                capture_output=True,\n                **invisibledict\n            )\n            out = p.stdout.decode(\"utf-8\", \"ignore\")\n            tstamp = strftime(\"%Y_%m_%d_%H_%M_%S\")\n            df = pd.concat(\n                [df, pd.read_csv(io.StringIO(out)).assign(timestamp=tstamp)],\n                ignore_index=True,\n            )\n            print(df[len(df) - 1 :].to_string())\n            sleep(sleeptime)\n    except KeyboardInterrupt:\n        if savepath:\n            df.to_csv(savepath)\n    return df", "answer_comment": [], "answer_score": "5", "answer_code_list": ["import torch\ntorch.cuda.is_available()\n", "import io\nimport shutil\nimport subprocess\nfrom time import sleep, strftime\nimport pandas as pd\n\nstartupinfo = subprocess.STARTUPINFO()\nstartupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\nstartupinfo.wShowWindow = subprocess.SW_HIDE\ncreationflags = subprocess.CREATE_NO_WINDOW\ninvisibledict = {\n    \"startupinfo\": startupinfo,\n    \"creationflags\": creationflags,\n    \"start_new_session\": True,\n}\npath = shutil.which(\"nvidia-smi.exe\")\n\n\ndef nvidia_log(savepath=None, sleeptime=1):\n    \"\"\"\n        Monitor NVIDIA GPU information and log the data into a pandas DataFrame.\n\n        Parameters:\n            savepath (str, optional): The file path to save the log data as a CSV file.\n                                      If provided, the data will be saved upon KeyboardInterrupt.\n            sleeptime (int, optional): The time interval (in seconds) between each data logging.\n\n        Returns:\n            pandas.DataFrame: A DataFrame containing the logged NVIDIA GPU information with the following columns:\n                - index: GPU index.\n                - name: GPU name.\n                - memory.total [MiB]: Total GPU memory in MiB (Mebibytes).\n                - memory.used [MiB]: Used GPU memory in MiB (Mebibytes).\n                - memory.free [MiB]: Free GPU memory in MiB (Mebibytes).\n                - temperature.gpu: GPU temperature in Celsius.\n                - pstate: GPU performance state.\n                - utilization.gpu [%]: GPU utilization percentage.\n                - utilization.memory [%]: Memory utilization percentage.\n                - timestamp: Timestamp in the format \"YYYY_MM_DD_HH_MM_SS\".\n\n        Description:\n            This function uses the NVIDIA System Management Interface (nvidia-smi) to query GPU information,\n            including memory usage, temperature, performance state, and utilization. The data is collected\n            in real-time and logged into a pandas DataFrame. The logging continues indefinitely until a\n            KeyboardInterrupt (usually triggered by pressing Ctrl + C).\n\n            If the 'savepath' parameter is provided, the collected GPU information will be saved to a CSV\n            file when the monitoring is interrupted by the user (KeyboardInterrupt).\n\n            Note: This function is intended for systems with NVIDIA GPUs on Windows and requires the nvidia-smi.exe\n            executable to be available in the system path.\n\n        Example:\n            # Start monitoring NVIDIA GPU and display the real-time log\n            nvidia_log()\n\n            # Start monitoring NVIDIA GPU and save the log data to a CSV file\n            nvidia_log(savepath=\"gpu_log.csv\")\n\n            # Start monitoring NVIDIA GPU with a custom time interval between logs (e.g., 2 seconds)\n            nvidia_log(sleeptime=2)\n\n              index                            name  memory.total [MiB]  memory.used [MiB]  memory.free [MiB]  temperature.gpu  pstate  utilization.gpu [%]  utilization.memory [%]            timestamp\n    0     0   NVIDIA GeForce RTX 2060 SUPER            8192 MiB           1321 MiB           6697 MiB               45      P8                 16 %                     5 %  2023_07_18_11_52_55\n      index                            name  memory.total [MiB]  memory.used [MiB]  memory.free [MiB]  temperature.gpu  pstate  utilization.gpu [%]  utilization.memory [%]            timestamp\n    1     0   NVIDIA GeForce RTX 2060 SUPER            8192 MiB           1321 MiB           6697 MiB               44      P8                 17 %                     6 %  2023_07_18_11_52_56\n      index                            name  memory.total [MiB]  memory.used [MiB]  memory.free [MiB]  temperature.gpu  pstate  utilization.gpu [%]  utilization.memory [%]            timestamp\n    2     0   NVIDIA GeForce RTX 2060 SUPER            8192 MiB           1321 MiB           6697 MiB               44      P8                  2 %                     4 %  2023_07_18_11_52_57\n      index                            name  memory.total [MiB]  memory.used [MiB]  memory.free [MiB]  temperature.gpu  pstate  utilization.gpu [%]  utilization.memory [%]            timestamp\n    3     0   NVIDIA GeForce RTX 2060 SUPER            8192 MiB           1321 MiB           6697 MiB               44      P8                  4 %                     5 %  2023_07_18_11_52_58\n      index                            name  memory.total [MiB]  memory.used [MiB]  memory.free [MiB]  temperature.gpu  pstate  utilization.gpu [%]  utilization.memory [%]            timestamp\n    4     0   NVIDIA GeForce RTX 2060 SUPER            8192 MiB           1321 MiB           6697 MiB               46      P2                 22 %                     1 %  2023_07_18_11_52_59\n      index                            name  memory.total [MiB]  memory.used [MiB]  memory.free [MiB]  temperature.gpu  pstate  utilization.gpu [%]  utilization.memory [%]            timestamp\n    5     0   NVIDIA GeForce RTX 2060 SUPER            8192 MiB           1320 MiB           6698 MiB               45      P8                  0 %                     0 %  2023_07_18_11_53_00\n      index                            name  memory.total [MiB]  memory.used [MiB]  memory.free [MiB]  temperature.gpu  pstate  utilization.gpu [%]  utilization.memory [%]            timestamp\n    6     0   NVIDIA GeForce RTX 2060 SUPER            8192 MiB           1320 MiB           6698 MiB               45      P8                  2 %                     4 %  2023_07_18_11_53_01\n      index                            name  memory.total [MiB]  memory.used [MiB]  memory.free [MiB]  temperature.gpu  pstate  utilization.gpu [%]  utilization.memory [%]            timestamp\n    7     0   NVIDIA GeForce RTX 2060 SUPER            8192 MiB           1320 MiB           6698 MiB               44      P8                 12 %                     5 %  2023_07_18_11_53_02\n      index                            name  memory.total [MiB]  memory.used [MiB]  memory.free [MiB]  temperature.gpu  pstate  utilization.gpu [%]  utilization.memory [%]            timestamp\n    8     0   NVIDIA GeForce RTX 2060 SUPER            8192 MiB           1320 MiB           6698 MiB               44      P8                  3 %                     4 %  2023_07_18_11_53_03\n    \"\"\"\n    df = pd.DataFrame(\n        columns=[\n            \"index\",\n            \" name\",\n            \" memory.total [MiB]\",\n            \" memory.used [MiB]\",\n            \" memory.free [MiB]\",\n            \" temperature.gpu\",\n            \" pstate\",\n            \" utilization.gpu [%]\",\n            \" utilization.memory [%]\",\n            \"timestamp\",\n        ]\n    )\n    try:\n        while True:\n            p = subprocess.run(\n                [\n                    path,\n                    \"--query-gpu=index,gpu_name,memory.total,memory.used,memory.free,temperature.gpu,pstate,\"\n                    \"utilization.gpu,utilization.memory\",\n                    \"--format=csv\",\n                ],\n                capture_output=True,\n                **invisibledict\n            )\n            out = p.stdout.decode(\"utf-8\", \"ignore\")\n            tstamp = strftime(\"%Y_%m_%d_%H_%M_%S\")\n            df = pd.concat(\n                [df, pd.read_csv(io.StringIO(out)).assign(timestamp=tstamp)],\n                ignore_index=True,\n            )\n            print(df[len(df) - 1 :].to_string())\n            sleep(sleeptime)\n    except KeyboardInterrupt:\n        if savepath:\n            df.to_csv(savepath)\n    return df\n"], "is_accepted": false}, {"answer_content": "Obtain environment information using PyTorch via Terminal command\npython -m torch.utils.collect_env\n\nAnd if you can have True value for \"Is CUDA available\" in comand result like below, then your PyTorch is using GPU.\n\nIs CUDA available: True", "answer_comment": ["or in python or ipython using   `  torch.utils.collect_env.main()  `"], "answer_score": "5", "answer_code_list": ["python -m torch.utils.collect_env\n"], "is_accepted": false}, {"answer_content": "If you are here because your pytorch always gives False for torch.cuda.is_available() that's probably because you installed your pytorch version without GPU support. (Eg: you coded up in laptop then testing on server). \nThe solution is to uninstall and install pytorch again with the right command from pytorch downloads page. Also refer this pytorch issue.", "answer_comment": ["Even though what you have written is related to the question. The question is: \"How to check if pytorch is using the GPU?\" and not \"What can I do if PyTorch doesn't detect my GPU?\" So I would say that this answer does not really belong to this question. But you may find another question about this specific issue where you can share your knowledge. If not you could even write a question and answer it yourself to help others with the same issue!"], "answer_score": "4", "answer_code_list": [], "is_accepted": false}, {"answer_content": "It is possible for\ntorch.cuda.is_available()\n\nto return True but to get the following error when running\n>>> torch.rand(10).to(device)\n\nas suggested by MBT:\nRuntimeError: CUDA error: no kernel image is available for execution on the device\n\nThis link explains that\n\n... torch.cuda.is_available only checks whether your driver is compatible with the version of cuda we used in the binary. So it means that CUDA 10.1 is compatible with your driver. But when you do computation with CUDA, it couldn't find the code for your arch.", "answer_comment": [], "answer_score": "2", "answer_code_list": ["torch.cuda.is_available()\n", ">>> torch.rand(10).to(device)\n", "RuntimeError: CUDA error: no kernel image is available for execution on the device\n"], "is_accepted": false}, {"answer_content": "You can just use the following code:\nimport torch\ntorch.cuda.is_available()\n\nif it returns True, it means the GPU is working, while False means that it does not.", "answer_comment": ["hello, how to typing the words with background?@vvvv"], "answer_score": "2", "answer_code_list": ["import torch\ntorch.cuda.is_available()\n"], "is_accepted": false}, {"answer_content": "option 1:\nimport torch\ntorch.cuda.get_device_properties('cuda')\n\nOutput:\n_CudaDeviceProperties(name='NVIDIA GeForce RTX 3060', major=8, minor=6, total_memory=12036MB, multi_processor_count=28)\noption 2:\nprint(torch.__config__.show())\n\noutput:\nPyTorch built with:\n\nC++ Version: 201703\nMSVC 192930154\nIntel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications\nIntel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)\nOpenMP 2019\nLAPACK is enabled (usually provided by MKL)\nCPU capability usage: AVX2\nBuild settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=C:/actions-runner/_work/pytorch/pytorch/builder/windows/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /Zc:__cplusplus /bigobj /FS /utf-8 -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE /wd4624 /wd4068 /wd4067 /wd4267 /wd4661 /wd4717 /wd4244 /wd4804 /wd4273, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=0, USE_CUDNN=OFF, USE_CUSPARSELT=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF,", "answer_comment": ["Thank you for your interest in contributing to the Stack Overflow community. This question already has quite a few answers\u2014including one that has been extensively validated by the community. Are you certain your approach hasn\u2019t been given previously? If so, it would be useful to explain how your approach is different, under what circumstances your approach might be preferred, and/or why you think the previous answers aren\u2019t sufficient. Can you kindly edit your answer to offer an explanation?"], "answer_score": "2", "answer_code_list": ["import torch\ntorch.cuda.get_device_properties('cuda')\n", "print(torch.__config__.show())\n"], "is_accepted": false}, {"answer_content": "Create a tensor on the GPU as follows:\n$ python\n>>> import torch\n>>> print(torch.rand(3,3).cuda()) \n\nDo not quit, open another terminal and check if the python process is using the GPU using:\n$ nvidia-smi", "answer_comment": ["I specifically asked for a solution that does not involve nvidia-smi from the command line", "Well, technically you can always parse the output any command-line tools, including nvidia-smi."], "answer_score": "0", "answer_code_list": ["$ python\n>>> import torch\n>>> print(torch.rand(3,3).cuda()) \n", "$ nvidia-smi\n"], "is_accepted": false}, {"answer_content": "Most answers above shows how you can check the cuda availability which is important. But my understanding what you need is to see if you actually leverage the GPU. I suggest to check the which device contains the tensors you are processing.\nmytensor.get_device()\nhttps://pytorch.org/docs/stable/generated/torch.Tensor.get_device.html", "answer_comment": [], "answer_score": "0", "answer_code_list": [], "is_accepted": false}, {"answer_content": "Using the code below\nimport torch\ntorch.cuda.is_available()\n\nwill only display whether the GPU is present and detected by pytorch or not.\nBut in the \"task manager-> performance\" the GPU utilization will be very few percent.\nWhich means you are actually running using CPU.\nTo solve the above issue check and change:\n\nGraphics setting --> Turn on Hardware accelerated GPU settings, restart.\nOpen NVIDIA control panel --> Desktop --> Display GPU in the notification area\n[Note: If you have newly installed windows then you also have to agree the terms and conditions in NVIDIA control panel]\n\nThis should work!", "answer_comment": ["The task manager is a very bad way of determining GPU usage actually, see here: stackoverflow.com/questions/69791848/\u2026"], "answer_score": "-1", "answer_code_list": ["import torch\ntorch.cuda.is_available()\n"], "is_accepted": false}, {"answer_content": "step 1: import torch library\nimport torch\n\n#step 2: create tensor\ntensor = torch.tensor([5, 6])\n\n#step 3: find the device type\n#output 1: in the below, the output we can get the size(tensor.shape), dimension(tensor.ndim),\n#and device on which the tensor is processed\ntensor, tensor.device, tensor.ndim, tensor.shape\n\n(tensor([5, 6]), device(type='cpu'), 1, torch.Size([2]))\n\n#or\n#output 2: in the below, the output we can get the only device type\ntensor.device\n\ndevice(type='cpu')\n\n#As my system using cpu processor \"11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz   2.42 GHz\"\n#find, if the tensor processed GPU?\nprint(tensor, torch.cuda.is_available()\n\n# the output will be\n\ntensor([5, 6]) False \n\n#above output is false, hence it is not on gpu\n#happy coding :)", "answer_comment": ["torch.cuda.is_available() can still be true and tensor.device set to cpu."], "answer_score": "-1", "answer_code_list": ["import torch\n", "tensor = torch.tensor([5, 6])\n", "tensor, tensor.device, tensor.ndim, tensor.shape\n\n(tensor([5, 6]), device(type='cpu'), 1, torch.Size([2]))\n", "tensor.device\n\ndevice(type='cpu')\n", "print(tensor, torch.cuda.is_available()\n\n# the output will be\n\ntensor([5, 6]) False \n"], "is_accepted": false}], "views": "1.3m", "title": "How do I check if PyTorch is using the GPU?", "question_link": "https://stackoverflow.com/questions/48152674/how-do-i-check-if-pytorch-is-using-the-gpu", "question_content": "How do I check if PyTorch is using the GPU? The nvidia-smi command can detect GPU activity, but I want to check it directly from inside a Python script.", "question_comment": ["is there a way to get a list of all currently available gpus? something like devices = torch.get_all_devices() # [0, 1, 2] or whatever their name is", "See stackoverflow.com/questions/64776822/\u2026: [torch.cuda.device(i) for i in range(torch.cuda.device_count())]", "I was told this works list(range(torch.cuda.device_count())). Thanks though!", "@CharlieParker,   You'd want (assuming you've import torch):   devices = [d for d in range(torch.cuda.device_count())]  And if you want the names:  device_names  = [torch.cuda.get_device_name(d) for d in devices]  You may, like me, like to map these as dict for cross machine management:   device_to_name = dict( device_names, devices )", "Use this - pypi.org/project/test-pytorch-gpu"]}]